"""
HACCP RAG í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •

ì‚¬ìš© ë°©ë²•:
1. í™˜ê²½ë³€ìˆ˜ HACCP_DATA_ROOT ì„¤ì • (ì„ íƒ)
   export HACCP_DATA_ROOT=/path/to/00_data

2. ë˜ëŠ” ì´ íŒŒì¼ì—ì„œ ì§ì ‘ DATA_ROOT ìˆ˜ì •

ê²½ë¡œ êµ¬ì¡°:
00_data/
  â”œâ”€â”€ input/                       â† ì›ë³¸ ë°ì´í„° ë° ì¸ë±ìŠ¤ (ì½ê¸° ì „ìš©)
  â”‚   â”œâ”€â”€ raw_law/ë²•ë¥ íŒŒì¼ì›ë³¸(pdf)/
  â”‚   â”œâ”€â”€ raw_manual/
  â”‚   â””â”€â”€ indexes/                 â† ì¸ë±ìŠ¤ (ë‚ ì§œë³„ ë²„ì „ ê´€ë¦¬)
  â”‚       â”œâ”€â”€ law/
  â”‚       â”‚   â””â”€â”€ YYYY-MM-DD/      â† ë²•ë¥  ì¸ë±ìŠ¤ (ë‚ ì§œë³„)
  â”‚       â”‚       â”œâ”€â”€ idx_ì‹í’ˆìœ„ìƒë²•/
  â”‚       â”‚       â”œâ”€â”€ idx_ì¶•ì‚°ë¬¼ ìœ„ìƒê´€ë¦¬ë²•/
  â”‚       â”‚       â””â”€â”€ ...
  â”‚       â””â”€â”€ manual/
  â”‚           â””â”€â”€ YYYY-MM-DD/      â† ë§¤ë‰´ì–¼ ì¸ë±ìŠ¤ (ë‚ ì§œë³„)
  â”‚               â”œâ”€â”€ idx_1. íš¨ìœ¨ì ì¸.../
  â”‚               â”œâ”€â”€ idx_11. HACCP.../
  â”‚               â””â”€â”€ ...
  â””â”€â”€ output/                      â† ìƒì„± ë°ì´í„° (ì“°ê¸°)
      â”œâ”€â”€ logs/                    â† result.txt, chat_log.txt
      â”œâ”€â”€ training_data/           â† triplets_group_bgem3.jsonl
      â”œâ”€â”€ benchmark/               â† benchmark_result/
      â””â”€â”€ finetuning/              â† íŒŒì¸íŠœë‹ ëª¨ë¸ (Gitì—ì„œ ì œì™¸)
"""

import os
from pathlib import Path

# ===== ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ ì„¤ì • =====
# ìš°ì„ ìˆœìœ„: í™˜ê²½ë³€ìˆ˜ > ìˆ˜ë™ ì„¤ì • > ê¸°ë³¸ê°’(í”„ë¡œì íŠ¸ ë£¨íŠ¸)
DATA_ROOT = os.getenv(
    "HACCP_DATA_ROOT",
    str(Path(__file__).resolve().parent / "00_data")  # ê¸°ë³¸: í”„ë¡œì íŠ¸ ë‚´ 00_data/
)

DATA_ROOT = Path(DATA_ROOT)

# ===== INPUT: ì›ë³¸ ë°ì´í„° ë° ì¸ë±ìŠ¤ (ì½ê¸° ì „ìš©) =====
INPUT_DIR = DATA_ROOT / "input"

# ì›ë³¸ ë°ì´í„° (01_preprocessì™€ ë§¤í•‘)
RAW_LAW_DIR = INPUT_DIR / "raw_law" / "ë²•ë¥ íŒŒì¼ì›ë³¸(pdf)"
RAW_MANUAL_HTML_DIR = INPUT_DIR / "raw_manual" / "ë§¤ë‰´ì–¼_1ì°¨_ì „ì²˜ë¦¬(html_to_blocks)"
RAW_MANUAL_JSON_DIR = INPUT_DIR / "raw_manual" / "ë§¤ë‰´ì–¼_1ì°¨_ì „ì²˜ë¦¬ê²°ê³¼ë¬¼(jsoníŒŒì¼ëª¨ìŒ)"

# ì¸ë±ìŠ¤ (02_ragì—ì„œ ì°¸ì¡°)
# ë‚ ì§œë³„ ì¸ë±ìŠ¤ êµ¬ì¡°: indexes/law/YYYY-MM-DD/, indexes/manual/YYYY-MM-DD/
# ì‹¤ì œ ì‚¬ìš© ì‹œ run_law_final.py, run_manual_final.pyì—ì„œ ë‚ ì§œë¥¼ ì§€ì •í•˜ì—¬ ë¡œë“œ
IDX_LAW_DIR = INPUT_DIR / "indexes" / "law"
IDX_MANUAL_DIR = INPUT_DIR / "indexes" / "manual"

# ===== OUTPUT: ìƒì„± ë°ì´í„° (ì“°ê¸°) =====
OUTPUT_DIR = DATA_ROOT / "output"

# ë¡œê·¸ íŒŒì¼
LOG_DIR = OUTPUT_DIR / "logs"
RESULT_FILE = LOG_DIR / "result.txt"
CHAT_LOG_FILE = LOG_DIR / "chat_log.txt"

# í•™ìŠµ ë°ì´í„°
TRAINING_DATA_DIR = OUTPUT_DIR / "training_data"
TRIPLET_FILE = TRAINING_DATA_DIR / "triplets_group_bgem3.jsonl"

# ë²¤ì¹˜ë§ˆí¬
BENCHMARK_RESULT_DIR = OUTPUT_DIR / "benchmark_result"

# íŒŒì¸íŠœë‹ (Gitì—ì„œ ì œì™¸)
FINETUNING_DIR = OUTPUT_DIR / "finetuning"
FINETUNED_MODEL_DIR = FINETUNING_DIR / "finetuned_embedding_model"
CROSS_VALIDATION_DIR = FINETUNING_DIR / "cross_validation_ê²°ê³¼"

# ===== vLLM Client ì„¤ì • =====
# VARCO ëª¨ë¸ìš© í´ë¼ì´ì–¸íŠ¸ (ì§€ì—° ë¡œë”©)
VLLM_VARCO_BASE_URL = os.getenv("VLLM_VARCO_BASE_URL", "http://localhost:8400/v1")

# ===== ì„ë² ë”© ì„œë²„ ì„¤ì • =====
# bge-m3 ì„ë² ë”© ëª¨ë¸ ì›ê²© ì„œë²„ ì‚¬ìš© ì—¬ë¶€
# Docker ê¶Œí•œ ë¬¸ì œë¡œ ë¡œì»¬ ëª¨ë“œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
USE_REMOTE_EMBEDDING = os.getenv("USE_REMOTE_EMBEDDING", "false").lower() == "true"  # ê¸°ë³¸ê°’ false (ë¡œì»¬ ëª¨ë“œ)
# ì„ë² ë”© ì„œë²„ URL (TEI ë˜ëŠ” ì»¤ìŠ¤í…€ ì„œë²„)
EMBEDDING_SERVER_URL = os.getenv("EMBEDDING_SERVER_URL", "http://localhost:8401")

# OpenAI client ì´ˆê¸°í™”ëŠ” í•„ìš” ì‹œì ì— ìˆ˜í–‰ (ì§€ì—° ë¡œë”©)
_vllm_varco_client = None
_vllm_embed_client = None

def get_vllm_varco_client():
    """VARCO ëª¨ë¸ìš© vLLM í´ë¼ì´ì–¸íŠ¸ (ì§€ì—° ë¡œë”©)"""
    global _vllm_varco_client
    if _vllm_varco_client is None:
        try:
            from openai import OpenAI
            _vllm_varco_client = OpenAI(
                base_url=VLLM_VARCO_BASE_URL,
                api_key="EMPTY"
            )
        except ImportError:
            raise ImportError("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install openai")
    return _vllm_varco_client

def get_embedding_client():
    """ì„ë² ë”© ì„œë²„ í´ë¼ì´ì–¸íŠ¸ (ì§€ì—° ë¡œë”©)"""
    global _vllm_embed_client
    if _vllm_embed_client is None:
        try:
            import requests
            _vllm_embed_client = requests.Session()
        except ImportError:
            raise ImportError("requests íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install requests")
    return _vllm_embed_client

def remote_embed(texts, normalize=True):
    """
    ì›ê²© ì„ë² ë”© ì„œë²„ì—ì„œ ì„ë² ë”© ìƒì„±
    
    Args:
        texts: ë¬¸ìì—´ ë˜ëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        normalize: ì •ê·œí™” ì—¬ë¶€ (ê¸°ë³¸ True)
    
    Returns:
        numpy.ndarray: ì„ë² ë”© ë²¡í„° (ë‹¨ì¼ í…ìŠ¤íŠ¸) ë˜ëŠ” ë²¡í„° ë°°ì—´ (ë¦¬ìŠ¤íŠ¸)
    """
    import numpy as np
    
    client = get_embedding_client()
    
    # ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    is_single = isinstance(texts, str)
    if is_single:
        texts = [texts]
    
    try:
        # TEI í˜¸í™˜ API í˜¸ì¶œ
        response = client.post(
            f"{EMBEDDING_SERVER_URL}/embed",
            json={"inputs": texts, "normalize": normalize},
            timeout=30
        )
        response.raise_for_status()
        embeddings = np.array(response.json())
        
        # ë‹¨ì¼ í…ìŠ¤íŠ¸ë©´ ì²« ë²ˆì§¸ ë²¡í„°ë§Œ ë°˜í™˜
        return embeddings[0] if is_single else embeddings
    
    except Exception as e:
        print(f"âŒ ì›ê²© ì„ë² ë”© ì„œë²„ ìš”ì²­ ì‹¤íŒ¨: {e}")
        print(f"   ì„œë²„ URL: {EMBEDDING_SERVER_URL}")
        raise


class EmbeddingModelWrapper:
    """
    ë¡œì»¬/ì›ê²© ì„ë² ë”© ëª¨ë¸ì„ íˆ¬ëª…í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ë˜í¼
    
    SentenceTransformerì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
    """
    def __init__(self, local_model=None, use_remote=False):
        self.local_model = local_model
        self.use_remote = use_remote
    
    def encode(self, texts, normalize_embeddings=True, convert_to_tensor=False, **kwargs):
        """
        SentenceTransformer.encode() í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤
        """
        import numpy as np
        
        if self.use_remote:
            # ì›ê²© ì„ë² ë”© ì„œë²„ ì‚¬ìš©
            embeddings = remote_embed(texts, normalize=normalize_embeddings)
        else:
            # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
            if self.local_model is None:
                raise RuntimeError("ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            embeddings = self.local_model.encode(
                texts, 
                normalize_embeddings=normalize_embeddings,
                convert_to_tensor=convert_to_tensor,
                **kwargs
            )
        
        if convert_to_tensor:
            import torch
            return torch.from_numpy(embeddings) if isinstance(embeddings, np.ndarray) else embeddings
        
        return embeddings

# ===== ê¸°íƒ€ =====
PROJECT_ROOT = Path(__file__).resolve().parent

# ===== ê²½ë¡œ ì¡´ì¬ í™•ì¸ í•¨ìˆ˜ =====
def check_paths():
    """í•„ìˆ˜ ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    required_paths = {
        "ì›ë³¸ ë²•ë¥  ë°ì´í„°": RAW_LAW_DIR,
        "ì›ë³¸ ë§¤ë‰´ì–¼ JSON": RAW_MANUAL_JSON_DIR,
        "ë²•ë¥  ì¸ë±ìŠ¤ ë£¨íŠ¸": IDX_LAW_DIR,
        "ë§¤ë‰´ì–¼ ì¸ë±ìŠ¤ ë£¨íŠ¸": IDX_MANUAL_DIR,
    }
    
    missing = []
    for name, path in required_paths.items():
        if not path.exists():
            missing.append(f"  - {name}: {path}")
    
    if missing:
        print("âš ï¸  ë‹¤ìŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:")
        print("\n".join(missing))
        print(f"\nğŸ’¡ DATA_ROOT ì„¤ì •: {DATA_ROOT}")
        print("   í™˜ê²½ë³€ìˆ˜ HACCP_DATA_ROOTë¥¼ ì„¤ì •í•˜ê±°ë‚˜ config.pyë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")
        return False
    
    print(f"âœ… ëª¨ë“  í•„ìˆ˜ ê²½ë¡œ í™•ì¸ ì™„ë£Œ (DATA_ROOT: {DATA_ROOT})")
    return True


def ensure_output_dirs():
    """Output í´ë” ìë™ ìƒì„±"""
    dirs_to_create = [
        LOG_DIR,
        TRAINING_DATA_DIR,
        BENCHMARK_RESULT_DIR,
        FINETUNING_DIR,
    ]
    
    for d in dirs_to_create:
        d.mkdir(parents=True, exist_ok=True)
    
    print(f"âœ… Output ë””ë ‰í„°ë¦¬ ì¤€ë¹„ ì™„ë£Œ")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš©: python config.py ì‹¤í–‰ ì‹œ ê²½ë¡œ í™•ì¸
    print("=" * 70)
    print("HACCP RAG ê²½ë¡œ ì„¤ì •")
    print("=" * 70)
    print(f"DATA_ROOT: {DATA_ROOT}\n")
    
    print("ğŸ“¥ INPUT (ì›ë³¸ ë°ì´í„°, ì½ê¸° ì „ìš©)")
    print(f"  - ë²•ë¥  ì›ë³¸: {RAW_LAW_DIR}")
    print(f"  - ë§¤ë‰´ì–¼ HTML: {RAW_MANUAL_HTML_DIR}")
    print(f"  - ë§¤ë‰´ì–¼ JSON: {RAW_MANUAL_JSON_DIR}")
    print(f"  - ë²•ë¥  ì¸ë±ìŠ¤: {IDX_LAW_DIR}")
    print(f"  - ë§¤ë‰´ì–¼ ì¸ë±ìŠ¤: {IDX_MANUAL_DIR}\n")
    
    print("ğŸ“¤ OUTPUT (ìƒì„± ë°ì´í„°, ì“°ê¸°)")
    print(f"  - ë¡œê·¸: {LOG_DIR}")
    print(f"  - í•™ìŠµ ë°ì´í„°: {TRAINING_DATA_DIR}")
    print(f"  - ë²¤ì¹˜ë§ˆí¬: {BENCHMARK_DIR}")
    print(f"  - íŒŒì¸íŠœë‹: {FINETUNING_DIR}\n")
    
    print("=" * 70)
    check_paths()
    print()
    ensure_output_dirs()
