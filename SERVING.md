# SERVING

**ì›¹ì‚¬ì´íŠ¸ ì½”ë“œ ìˆ˜ì • í•„ìš” ì—¬ë¶€: ì—†ìŒ**
- API ì—”ë“œí¬ì¸íŠ¸ ë™ì¼ (`POST /queries`, `POST /answers` ë“±)
- Request/Response í˜•ì‹ ë™ì¼
- ë‚´ë¶€ ë¡œì§ë§Œ ë³€ê²½ (FastAPI â†’ vLLM í˜¸ì¶œ ì¶”ê°€)
- **ì›¹ì‚¬ì´íŠ¸ëŠ” ê¸°ì¡´ì²˜ëŸ¼ `http://localhost:8000`ë§Œ í˜¸ì¶œí•˜ë©´ ë¨**

**ì„œë²„ ì‹¤í–‰ ë°©ë²•:**
```bash
cd 03_api
python start_services.py  # 3ê°œ ì„œë²„ ìë™ ì‹¤í–‰
```

**ë³€ê²½ ì‚¬í•­:**
- ë‹µë³€ ìƒì„± ì†ë„: 30ì´ˆ â†’ 8-12ì´ˆ
- ì„œë²„ ê°œìˆ˜: 1ê°œ â†’ 3ê°œ (ì„ë² ë”©, FastAPI, vLLM)

---

## ğŸ“– ë°±ì—”ë“œ ê°œë°œììš©

**ë¬¸ì œ**: ë‹µë³€ ìƒì„± 30ì´ˆ ì´ìƒ ì†Œìš”  
**í•´ê²°**: LLM ëª¨ë¸ì„ ë³„ë„ vLLM ì„œë²„ë¡œ ë¶„ë¦¬  
**ê²°ê³¼**: ë‹µë³€ ìƒì„± 8-12ì´ˆ (66% ë‹¨ì¶•)

### **ìµœì¢… ì•„í‚¤í…ì²˜**
```
â‘  FastAPI (500MB) - API ìš”ì²­ ì²˜ë¦¬, ê²€ìƒ‰ ìˆ˜í–‰
â‘¡ ì„ë² ë”© ì„œë²„ (1.5GB) - í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
â‘¢ vLLM ì„œë²„ (26GB) - LLM ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
```

**ì‹¤í–‰ ë°©ë²•**: `python start_services.py` (ì„ë² ë”© â†’ FastAPI â†’ vLLM ìˆœì„œë¡œ 3ê°œ í”„ë¡œì„¸ìŠ¤ ìë™ ì‹¤í–‰)

---

## Serving framework ì‚¬ìš© ëª©ì 

**ê¸°ì¡´ì˜ ë°©ë²•**

**03_api/services/query_service.py** ì°¸ê³ 
```python
# Boot Once: ëª¨ë“ˆ ë¡œë“œ ì‹œ 1íšŒë§Œ ì´ˆê¸°í™”
_rag_instance = None

def get_rag_instance() -> RAGAdapter:
    """RAG ì¸ìŠ¤í„´ìŠ¤ ì§€ì—° ë¡œë”© (ì•± ì „ì²´ì—ì„œ 1ê°œë§Œ ì‚¬ìš©)"""
    global _rag_instance
    if _rag_instance is None:
        _rag_instance = RAGAdapter()
    return _rag_instance
```

> FastAPI í”„ë¡œì„¸ìŠ¤ ë‚´ì—ì„œ ì „ì—­ ë³€ìˆ˜ë¡œ RAG ëª¨ë¸ ì‚¬ìš© ì¤‘  
> FastAPIëŠ” ë™ê¸° ì²˜ë¦¬ ê¸°ë°˜ìœ¼ë¡œ LLM ì¶”ë¡  ì¤‘ ë‹¤ë¥¸ ìš”ì²­ ì²˜ë¦¬ ë¶ˆê°€  
> vLLM í”„ë ˆì„ì›Œí¬ëŠ” KV ìºì‹œ, ë°°ì¹˜ ì²˜ë¦¬, continuous batching ë“±ìœ¼ë¡œ ì¶”ë¡  ì†ë„ ê°œì„ 


## Serving framework ì ìš©


**ì•„í‚¤í…ì²˜ ë¹„êµ**
```bash
# ê¸°ì¡´ (ëŠë¦¬ê³  ë¬´ê±°ì›€ âŒ)

í´ë¼ì´ì–¸íŠ¸ (ì›¹)
    â†“
FastAPI ì„œë²„ 1ê°œ (28GB)
â”œâ”€ KeyBERT (300MB)
â”œâ”€ bge-m3 (1.5GB)      â† ë¬´ê±°ì›€
â”œâ”€ VARCO LLM (26GB)    â† ë¬´ê±°ì›€
â””â”€ API ì²˜ë¦¬

ë¬¸ì œ:
- í•œ í”„ë¡œì„¸ìŠ¤ê°€ ëª¨ë“  ëª¨ë¸ ë¡œë“œ â†’ ë©”ëª¨ë¦¬ 28GB í•„ìš”
- LLM ì¶”ë¡  ì¤‘ ê²€ìƒ‰ ìš”ì²­ ì²˜ë¦¬ ë¶ˆê°€
- ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ

# ë³€ê²½ í›„ (ë¹ ë¥´ê³  íš¨ìœ¨ì  âœ…)

í´ë¼ì´ì–¸íŠ¸ (ì›¹)
    â†“
â‘  FastAPI (500MB) - HTTP ìš”ì²­ ë°›ê³  ì‘ë‹µ ë°˜í™˜
    â”œâ”€ ì„ë² ë”© í•„ìš” ì‹œ â†’ â‘¡ ì„ë² ë”© ì„œë²„ (1.5GB)ë¡œ POST ìš”ì²­
    â””â”€ ë‹µë³€ ìƒì„± ì‹œ â†’ â‘¢ vLLM ì„œë²„ (26GB)ë¡œ POST ìš”ì²­

ì¥ì :
âœ… ë…ë¦½ ì‹¤í–‰: ê° ì„œë²„ê°€ ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰
âœ… ë©”ëª¨ë¦¬ ì ˆì•½: FastAPIëŠ” LLM ë¡œë“œ ë¶ˆí•„ìš” (26GB ì ˆì•½)
âœ… ëª¨ë“ˆí™”: ì„œë²„ë³„ ì¬ì‹œì‘ ê°€ëŠ¥
âœ… í™•ì¥ì„±: ì„œë²„ë³„ ë…ë¦½ ìŠ¤ì¼€ì¼ë§ (ì˜ˆ: vLLM 2ëŒ€, FastAPI 1ëŒ€)
```

**ì†ŒìŠ¤ì½”ë“œ ë³€ê²½**

> API ì—”ë“œí¬ì¸íŠ¸ POST /answers ì˜ ê¸°ëŠ¥ë§Œ ì ìš©ë˜ë„ë¡ ìˆ˜ì •í•¨  
> í˜„ì¬ í”Œë¡œìš°ë¡œ ë™ì‘í•˜ê²Œ ë” ìµœì†Œí•œì˜ ì†ŒìŠ¤ì½”ë“œë§Œ ìˆ˜ì •í–ˆìœ¼ë‚˜ ì „ì²´ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³  ê·¸ì— ëŒ€í•œ ìˆ˜ì •ì‘ì—…ì´ í•„ìš”í•¨

**1. run_[law|manual]_final.py ì˜ load_llm()**

```python
def load_llm(model_id: str = LLM_MODEL_ID):
    """
    vLLM serving framework í™œìš©ì„ ìœ„í•´ ìˆ˜ì •ë¨
    
    ê¸°ì¡´ê³¼ ë‹¬ë¦¬ ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ê°™ì´ ë¡œë“œí•˜ì§€ ì•Šê³  í”„ë¡œì„¸ì„œë§Œ ë¡œë“œ
    """
    processor = AutoProcessor.from_pretrained(model_id)
    return None, processor
```

**2. run_[law|manual]_final.py ì˜ generate_llm_response()**

```python
def generate_llm_response(model, processor, conversation, max_new_tokens=1024):
    """
    vLLM serving framework í™œìš©ì„ ìœ„í•´ ìˆ˜ì •ë¨
    
    ëª¨ë¸ì— ì§ì ‘ ì…ë ¥í•˜ì§€ ì•Šê³  clientë¡œ localhost:8400/v1 (NCSoft ëª¨ë¸ ì—”ë“œí¬ì¸íŠ¸)ìœ¼ë¡œ ìš”ì²­ 
    """
    rendered_prompt = processor.apply_chat_template(
        conversation, add_generation_prompt=True, tokenize=False
    )

    input_len = len(processor.tokenizer(rendered_prompt)["input_ids"])
    
    # max_tokens ì•ˆì „í•˜ê²Œ ê³„ì‚°
    MAX_CONTEXT_LENGTH = 4096
    RESERVED_OUTPUT_TOKENS = 1024
    MIN_OUTPUT_TOKENS = 100
    
    available_tokens = MAX_CONTEXT_LENGTH - input_len
    max_tokens = min(available_tokens, RESERVED_OUTPUT_TOKENS)
    
    if max_tokens < MIN_OUTPUT_TOKENS:
        print(f"âš ï¸ ê²½ê³ : ì…ë ¥ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({input_len} tokens)")
        max_tokens = max(1, available_tokens)

    # vLLM í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© (config.pyì—ì„œ ê°€ì ¸ì˜´)
    client = get_vllm_varco_client()
    start = time.time()
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL_ID,
            messages=conversation,
            max_tokens=max_tokens
        )
        elapsed = time.time() - start
        output_text = response.choices[0].message.content
        return {"rendered_prompt": rendered_prompt, "output": output_text.strip(), "elapsed": elapsed}
    except Exception as e:
        elapsed = time.time() - start
        print(f"âŒ vLLM ì„œë²„ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return {"rendered_prompt": rendered_prompt, "output": f"[ì˜¤ë¥˜] vLLM ì„œë²„ ì‘ë‹µ ì‹¤íŒ¨: {str(e)}", "elapsed": elapsed}
```

**3. config.pyì— vLLM client í†µí•© (ì§€ì—° ë¡œë”© íŒ¨í„´)**

```python
# ===== vLLM Client ì„¤ì • =====
VLLM_VARCO_BASE_URL = os.getenv("VLLM_VARCO_BASE_URL", "http://localhost:8400/v1")

# ëª¨ë“ˆ ë ˆë²¨ ì „ì—­ ë³€ìˆ˜ (ì§€ì—° ì´ˆê¸°í™”)
_vllm_varco_client = None
_vllm_embed_client = None

def get_vllm_varco_client():
    """VARCO ëª¨ë¸ìš© vLLM í´ë¼ì´ì–¸íŠ¸ (ì§€ì—° ë¡œë”©)"""
    global _vllm_varco_client
    if _vllm_varco_client is None:
        from openai import OpenAI
        _vllm_varco_client = OpenAI(
            base_url=VLLM_VARCO_BASE_URL,
            api_key="EMPTY"
        )
    return _vllm_varco_client

def get_embedding_client():
    """ì„ë² ë”© ì„œë²„ í´ë¼ì´ì–¸íŠ¸ (ì§€ì—° ë¡œë”©)"""
    global _vllm_embed_client
    if _vllm_embed_client is None:
        import requests
        _vllm_embed_client = requests.Session()
    return _vllm_embed_client
```

**4. 03_api/adapters/rag_adapter.py**

```python
class RAGAdapter:
    def generate_answer(self, question: str, selected_categories: List[str], scope: str = "all"):
        # ===== 2. LLM ìƒì„± ì‹œê°„ ì¸¡ì • =====
        generation_start = time.time()
        
        # vLLM ì„œë²„ ì‚¬ìš© (ì¡°ê±´ ë¶„ê¸° ì—†ì´ í•­ìƒ í˜¸ì¶œ)
        answer = ""
        answer = self._generate_llm_answer(question, top_results, primary_type)
        answer = "LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤." if not answer else answer
        
        generation_end = time.time()
        generation_ms = int((generation_end - generation_start) * 1000)
```

**5. 02_rag/haccp_rag.pyì˜ GlobalBoot**

```python
class GlobalBoot:
    def __init__(self):
        # 1) LLM í”„ë¡œì„¸ì„œ ë¡œë“œ (vLLM ì„œë²„ ì‚¬ìš©ì„ ìœ„í•´ ëª¨ë¸ì€ ë¡œë“œí•˜ì§€ ì•ŠìŒ)
        self.llm_model = None  # vLLM ì„œë²„ ì‚¬ìš©ìœ¼ë¡œ ëª¨ë¸ ì§ì ‘ ë¡œë“œ ë¶ˆí•„ìš”
        self.llm_processor = None
        self.use_llm = True  # vLLM ì„œë²„ê°€ êµ¬ë™ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
        
        try:
            # í”„ë¡œì„¸ì„œë§Œ ë¡œë“œ
            _, self.llm_processor = self.RM.load_llm(self.RM.LLM_MODEL_ID)
            print(f"[INFO] LLM í”„ë¡œì„¸ì„œ ë¡œë“œ ì™„ë£Œ")
            print(f"[INFO] vLLM ì„œë²„ ì‚¬ìš© (ëª¨ë¸ ì§ì ‘ ë¡œë“œ ìƒëµ)")
        except Exception as e:
            print(f"[ê²½ê³ ] LLM í”„ë¡œì„¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.use_llm = False
        
        # 2) ë§¤ë‰´ì–¼ RAG ì¸ë±ìŠ¤ ì¼ê´„ ë¡œë“œ...
        # 3) ë²•ë¥  RAG ì¸ë±ìŠ¤ ì¼ê´„ ë¡œë“œ...
```

---
# ì‹¤ì œ êµ¬í˜„
## Phase 1 ì™„ë£Œ âœ…

### ë¬´ì—‡ì´ ë¬¸ì œì˜€ë‚˜?
ì´ì „ì—ëŠ” FastAPI ì„œë²„ í•˜ë‚˜ê°€ ëª¨ë“  ì¼ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , ê±°ëŒ€í•œ AI ëª¨ë¸(26GB í¬ê¸°)ì„ ì‚¬ìš©í•´ì„œ ë‹µë³€ë„ ìƒì„±í–ˆì£ . ì´ë ‡ê²Œ í•˜ë‚˜ì˜ ì„œë²„ê°€ ëª¨ë“  ê±¸ í•˜ë‹ˆ ë‘ ê°€ì§€ ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤:
1. ì„œë²„ê°€ ë„ˆë¬´ ë¬´ê±°ì›Œì„œ ë©”ëª¨ë¦¬ë¥¼ 28GBë‚˜ ì¡ì•„ë¨¹ì—ˆìŠµë‹ˆë‹¤
2. AIê°€ ë‹µë³€ì„ ë§Œë“œëŠ” ë™ì•ˆ(30ì´ˆ) ë‹¤ë¥¸ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤

### ì–´ë–»ê²Œ í•´ê²°í–ˆë‚˜?
AI ëª¨ë¸ì„ ë³„ë„ ì„œë²„ë¡œ ë¶„ë¦¬í–ˆìŠµë‹ˆë‹¤. ì´ì œ:
- **FastAPI ì„œë²„**: ì§ˆë¬¸ì„ ë°›ê³  ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤ (ê°€ë²¼ìš´ ì¼ë§Œ)
- **vLLM ì„œë²„**: AI ëª¨ë¸ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤ (ë¬´ê±°ìš´ ì¼ë§Œ)

FastAPIê°€ ë‹µë³€ì´ í•„ìš”í•˜ë©´ vLLM ì„œë²„ì— "ì´ ë¬¸ì„œë“¤ë¡œ ë‹µë³€ ë§Œë“¤ì–´ì¤˜"ë¼ê³  HTTP ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.

### ê²°ê³¼ëŠ”?
- FastAPI ì„œë²„ê°€ 26GB ê°€ë²¼ì›Œì¡ŒìŠµë‹ˆë‹¤ (28GB â†’ 2GB)
- ê²€ìƒ‰ê³¼ ë‹µë³€ ìƒì„±ì„ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤
- ë‹µë³€ ìƒì„± ì‹œê°„ì´ 30ì´ˆì—ì„œ 8-12ì´ˆë¡œ ì¤„ì—ˆìŠµë‹ˆë‹¤

---

## Phase 2 ì™„ë£Œ âœ…

### ë¬´ì—‡ì´ ë¬¸ì œì˜€ë‚˜?
KeyBERTë¼ëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ ë„êµ¬ê°€ ìˆìŠµë‹ˆë‹¤. ì´ê²Œ 300MB ì •ë„ ë˜ëŠ”ë°, ë²•ë¥  ê²€ìƒ‰ ëª¨ë“ˆê³¼ ë§¤ë‰´ì–¼ ê²€ìƒ‰ ëª¨ë“ˆì—ì„œ ê°ê° ë”°ë¡œ ë¡œë“œí•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤. ë˜‘ê°™ì€ ë„êµ¬ë¥¼ ë‘ ë²ˆ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ë‹ˆê¹Œ 600MBë¥¼ ì“°ê³  ìˆì—ˆì£ .

### ì–´ë–»ê²Œ í•´ê²°í–ˆë‚˜?
í”„ë¡œê·¸ë¨ ì‹œì‘í•  ë•Œ(GlobalBoot) KeyBERTë¥¼ ë”± í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ê±¸ ë²•ë¥  ëª¨ë“ˆê³¼ ë§¤ë‰´ì–¼ ëª¨ë“ˆ ë‘˜ ë‹¤ ê°™ì´ ì“°ë„ë¡ "ê³µìœ "ì‹œì¼°ìŠµë‹ˆë‹¤. ë„ì„œê´€ ì±…ì²˜ëŸ¼ í•œ ê¶Œì„ ì—¬ëŸ¬ ì‚¬ëŒì´ ëŒë ¤ë³´ëŠ” ê±°ì£ .

### ê²°ê³¼ëŠ”?
- ë©”ëª¨ë¦¬ë¥¼ 300MB ì ˆì•½í–ˆìŠµë‹ˆë‹¤ (600MB â†’ 300MB)
- í”„ë¡œê·¸ë¨ ì‹œì‘ ì†ë„ë„ ë¹¨ë¼ì¡ŒìŠµë‹ˆë‹¤

---

## Phase 3 ì™„ë£Œ âœ…

### ë¬´ì—‡ì´ ë¬¸ì œì˜€ë‚˜?
ì‚¬ìš©ì ì§ˆë¬¸ì„ ìˆ«ìë¡œ ë°”ê¿”ì£¼ëŠ” ì„ë² ë”© ëª¨ë¸(bge-m3, 1.5GB)ì´ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒë„ Phase 2ì²˜ëŸ¼ ë²•ë¥ /ë§¤ë‰´ì–¼ ëª¨ë“ˆì—ì„œ ê°ê° ë¡œë“œí•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤. ì´ 3GBë¥¼ ì“°ê³  ìˆì—ˆì£ .

### ì–´ë–»ê²Œ í•´ê²°í–ˆë‚˜?
ë‘ ê°€ì§€ë¥¼ í–ˆìŠµë‹ˆë‹¤:

**1. ì¤‘ë³µ ì œê±° (Phase 2ì™€ ë™ì¼)**
GlobalBootì—ì„œ ì„ë² ë”© ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³ , ë‘ ëª¨ë“ˆì´ ê³µìœ í•©ë‹ˆë‹¤.

**2. ì›ê²© ì„œë²„ ì‚¬ìš© (ì¶”ê°€ ê°œì„ )**
ì„ë² ë”© ì‘ì—…ì„ ì•„ì˜ˆ ë³„ë„ ì„œë²„(Docker ì»¨í…Œì´ë„ˆ)ë¡œ ë¹¼ëƒˆìŠµë‹ˆë‹¤. ì´ì œ FastAPIëŠ” "ì´ ë¬¸ì¥ ìˆ«ìë¡œ ë°”ê¿”ì¤˜"ë¼ê³  ìš”ì²­ë§Œ ë³´ë‚´ë©´ ë©ë‹ˆë‹¤. ë§Œì•½ Dockerê°€ ì—†ëŠ” í™˜ê²½ì´ë©´ ìë™ìœ¼ë¡œ ë¡œì»¬ ë°©ì‹ìœ¼ë¡œ ì „í™˜ë©ë‹ˆë‹¤.

### ê²°ê³¼ëŠ”?
- ê¸°ë³¸(ì¤‘ë³µ ì œê±°ë§Œ): 1.5GB ì ˆì•½
- ì›ê²© ì„œë²„ ì‚¬ìš© ì‹œ: ì¶”ê°€ë¡œ 1.5GB ì ˆì•½ (ì´ 3GB ì ˆì•½)
- FastAPI ì„œë²„ê°€ ë”ìš± ê°€ë²¼ì›Œì¡ŒìŠµë‹ˆë‹¤

---

## Phase 4 ì™„ë£Œ âœ…

### ë¬´ì—‡ì´ ë¬¸ì œì˜€ë‚˜?
ì´ì œ ì„œë²„ê°€ 3ê°œë¡œ ë‚˜ë‰˜ì—ˆìŠµë‹ˆë‹¤ (FastAPI, ì„ë² ë”©, vLLM). ë¬¸ì œëŠ” ì´ê±¸ ì‹¤í–‰í•˜ë ¤ë©´:
1. í„°ë¯¸ë„ ì°½ 3ê°œë¥¼ ì—´ê³ 
2. ê°ê° ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ê³ 
3. ê° ì„œë²„ê°€ ì œëŒ€ë¡œ ì‹œì‘ëëŠ”ì§€ ì¼ì¼ì´ í™•ì¸í•´ì•¼ í–ˆìŠµë‹ˆë‹¤

í•œ ë²ˆ ì‹¤í–‰í•˜ëŠ”ë° 5ë¶„ì´ ê±¸ë¦¬ê³ , ìˆœì„œë¥¼ í‹€ë¦¬ë©´ ì—ëŸ¬ê°€ ë‚¬ìŠµë‹ˆë‹¤.

### ì–´ë–»ê²Œ í•´ê²°í–ˆë‚˜?
`start_services.py`ë¼ëŠ” ìë™í™” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì´ê²Œ í•˜ëŠ” ì¼:

1. **ì„ë² ë”© ì„œë²„ ë¨¼ì € ì‹œì‘** (Docker ì»¨í…Œì´ë„ˆë¡œ ì‹¤í–‰, ì—†ìœ¼ë©´ ë¡œì»¬ ëª¨ë“œë¡œ ì „í™˜)
2. **FastAPI ì„œë²„ ì‹œì‘** (ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰)
3. **FastAPIê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°** (5ì´ˆë§ˆë‹¤ `/health` ì²´í¬)
4. **vLLM ì„œë²„ ì‹œì‘** (AI ëª¨ë¸ ë¡œë“œ)

ì´ì œ `python start_services.py` ëª…ë ¹ì–´ í•˜ë‚˜ë©´ 3ê°œ ì„œë²„ê°€ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ìë™ ì‹¤í–‰ë©ë‹ˆë‹¤. ì¤‘ê°„ì— ë¬¸ì œê°€ ìƒê¸°ë©´ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì•Œë ¤ì¤ë‹ˆë‹¤.

Ctrl+Cë¥¼ ëˆ„ë¥´ë©´ 3ê°œ ì„œë²„ê°€ ëª¨ë‘ ê¹”ë”í•˜ê²Œ ì¢…ë£Œë©ë‹ˆë‹¤.

### ê²°ê³¼ëŠ”?
- ì‹¤í–‰ ëª…ë ¹ì–´: 3ê°œ â†’ 1ê°œ
- ì‹¤í–‰ ì‹¤íŒ¨ ìœ„í—˜: ê±°ì˜ ì—†ìŒ
---

## ì „ì²´ ì‘ì—… ì™„ë£Œ!
### ğŸ“Š ìµœì¢… ê°œì„  ì‚¬í•­ ìš”ì•½

| í•­ëª© | ë³€ê²½ ì „ | ë³€ê²½ í›„ | íš¨ê³¼ |
|------|---------|---------|------|
| **LLM ì„œë¹™** | FastAPI í”„ë¡œì„¸ìŠ¤ ë‚´ë¶€ | vLLM ë³„ë„ í”„ë¡œì„¸ìŠ¤ | ë‹µë³€ ìƒì„± 18-22ì´ˆ ë‹¨ì¶• |
| **OpenAI Client** | run_law/run_manual ê°ê° ìƒì„± | config.py í•¨ìˆ˜ë¡œ í†µí•© | ì¤‘ë³µ ì½”ë“œ ì œê±° |
| **KeyBERT** | 2íšŒ ë¡œë“œ (ê° 300MB) | 1íšŒ ë¡œë“œ í›„ ì£¼ì… | 300MB ì ˆì•½ |
| **bge-m3** | 2íšŒ ë¡œë“œ (ê° 1.5GB) | 1íšŒ ë¡œë“œ í›„ ì£¼ì… | 1.5GB ì ˆì•½ |
| **max_tokens** | ê³ ì •ê°’ ì‚¬ìš© | ì…ë ¥ ê¸¸ì´ ê¸°ë°˜ ê³„ì‚° | í† í° ì´ˆê³¼ ì—ëŸ¬ ë°©ì§€ |
| **ì„œë²„ ê¸°ë™** | 3ê°œ í„°ë¯¸ë„ ìˆ˜ë™ ì‹¤í–‰ | start_services.py 1íšŒ ì‹¤í–‰ | ëª…ë ¹ì–´ 3ê°œ â†’ 1ê°œ |

**ì´ ë©”ëª¨ë¦¬ ì ˆì•½ (FastAPI í”„ë¡œì„¸ìŠ¤):**
- GlobalBoot 1íšŒ ë¡œë“œ: KeyBERT 300MB + bge-m3 1.5GB = 1.8GB
- ì›ê²© ì„ë² ë”© ì„œë²„ ì‚¬ìš©: 1.8GB + FastAPI ì„ë² ë”© ì œê±° 1.5GB = 3.3GB

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (Quick Start)
### **í•œ ì¤„ ëª…ë ¹ì–´ë¡œ 3ê°œ ì„œë²„ ëª¨ë‘ ìë™ ê¸°ë™!**

**Windows:**
```bash
cd 03_api
start_all.bat
```

**Linux/Mac:**
```bash
cd 03_api
bash start_all.sh
```

**ë˜ëŠ”:**
```bash
cd 03_api
python start_services.py
```

---
### **ìë™ ì‹¤í–‰ ìˆœì„œ**

```
1ï¸âƒ£ ì„ë² ë”© ì„œë²„ ì‹œì‘ (port 8401)
   â””â”€ docker run ëª…ë ¹ì–´ ì‹¤í–‰ (TEI ì»¨í…Œì´ë„ˆ)
   â””â”€ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ bge-m3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
   
2ï¸âƒ£ FastAPI ì„œë²„ ì‹œì‘ (port 8000)
   â””â”€ subprocess.Popen()ìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
   â””â”€ GlobalBoot: KeyBERT ë¡œë“œ â†’ ì¸ë±ìŠ¤ ë¡œë“œ â†’ RAGAdapter ìƒì„±
   
3ï¸âƒ£ RAGAdapter ì´ˆê¸°í™” ì™„ë£Œ ëŒ€ê¸°
   â””â”€ while ë£¨í”„ë¡œ http://localhost:8000/health ì²´í¬
   â””â”€ 200 ì‘ë‹µ ë°›ìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
   
4ï¸âƒ£ vLLM ì„œë²„ ìë™ ê¸°ë™ (port 8400)
   â””â”€ subprocess.Popen()ìœ¼ë¡œ vllm.entrypoints.openai.api_server ì‹¤í–‰
   â””â”€ VARCO-VISION-2.0-14B ëª¨ë¸ ë¡œë“œ (26GB)
```

**ì¢…ë£Œ:**
- `Ctrl+C` ì…ë ¥ â†’ signal_handler() í˜¸ì¶œ â†’ ê° í”„ë¡œì„¸ìŠ¤ì— terminate() ì „ì†¡ â†’ wait() ëŒ€ê¸°

---

### **ì „ì œ ì¡°ê±´**
- âœ… Docker ì„¤ì¹˜ í•„ìˆ˜ (ì„ë² ë”© ì„œë²„ìš©)
- âœ… GPU ë©”ëª¨ë¦¬ ìµœì†Œ 30GB ê¶Œì¥
- âœ… Python 3.8+
- âœ… CUDA 11.8+

---

## ğŸ“– ìˆ˜ë™ ê¸°ë™ (êµ¬ë²„ì „ ë°©ì‹)

### 1. vLLM ì„œë²„ ë¨¼ì € ê¸°ë™

```bash
python -m vllm.entrypoints.openai.api_server \
    --model "NCSOFT/VARCO-VISION-2.0-14B" \
    --host 0.0.0.0 \
    --port 8400 \
    --kv-cache-dtype auto \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8
```

### 2. ìƒˆ í„°ë¯¸ë„ì—ì„œ FastAPI ì„œë²„ ê¸°ë™

```bash
cd 03_api
python -m uvicorn main:app --host 0.0.0.0 --port 8000
```

**âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ:**
FastAPI ë¨¼ì € ë„ìš°ê¸° â†’ `/queries` ìš”ì²­ìœ¼ë¡œ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ â†’ vLLM ì„œë²„ ë„ìš°ê¸°

---

## ğŸ“Š ì„œë²„ ì ‘ì† ì •ë³´

### **3ê°œ ì„œë²„ êµ¬ì„±**

| ì„œë²„ | í¬íŠ¸ | ì—­í•  | ë©”ëª¨ë¦¬ |
|------|------|------|--------|
| **â‘  FastAPI** | 8000 | API ê²Œì´íŠ¸ì›¨ì´, ê²€ìƒ‰ ì¡°ìœ¨ | ~500MB |
| **â‘¡ ì„ë² ë”©** | 8401 | ì§ˆë¬¸ â†’ ìˆ«ì ë³€í™˜ (bge-m3) | ~1.5GB |
| **â‘¢ vLLM** | 8400 | ë‹µë³€ ìƒì„± (VARCO LLM) | ~26GB |

### **ì ‘ì† URL**
- **API**: http://localhost:8000
- **Swagger UI**: http://localhost:8000/docs
- **ì„ë² ë”©**: http://localhost:8401
- **vLLM**: http://localhost:8400/v1

### **ë¡œê·¸ í™•ì¸**
```bash
# ì„ë² ë”© ì„œë²„ (Docker)
docker logs tei-bge-m3

# FastAPI
cat 03_api/fastapi_server.log

# vLLM
cat 03_api/vllm_server.log
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°°í¬ ì „ í™•ì¸ì‚¬í•­

- [ ] Python í™˜ê²½ ì„¤ì • ì™„ë£Œ (Python 3.8+)
- [ ] í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ
  ```bash
  pip install fastapi uvicorn openai transformers sentence-transformers keybert torch faiss-cpu requests
  pip install vllm  # GPU ì„œë²„ì—ì„œ
  ```
- [ ] GPU ë©”ëª¨ë¦¬ ì¶©ë¶„í•œì§€ í™•ì¸ (ìµœì†Œ 16GB ê¶Œì¥)
- [ ] ì¸ë±ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
  - `00_data/input/indexes/law/2025-11-11/`
  - `00_data/input/indexes/manual/2025-11-11/`

### ì‹¤í–‰ í™•ì¸

- [ ] FastAPI ì„œë²„ ì •ìƒ ê¸°ë™ (`http://localhost:8000/health`)
- [ ] vLLM ì„œë²„ ì •ìƒ ê¸°ë™ (`http://localhost:8400/v1/models`)
- [ ] API í…ŒìŠ¤íŠ¸
  ```bash
  # 1ë‹¨ê³„: ì¿¼ë¦¬ ë“±ë¡
  curl -X POST "http://localhost:8000/queries" \
    -H "Content-Type: application/json" \
    -d '{"question": "HACCP ì¸ì¦ ê¸°ì¤€ì€?", "scope": "all"}'
  
  # 2ë‹¨ê³„: ë‹µë³€ ìƒì„±
  curl -X POST "http://localhost:8000/answers" \
    -H "Content-Type: application/json" \
    -d '{"query_id": "q_xxxxx", "selected_categories": ["LAW_ì „ì²´"]}'
  ```

### íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

**ë¬¸ì œ: FastAPI ì„œë²„ê°€ ì‹œì‘ë˜ì§€ ì•ŠìŒ**
- í•´ê²°: í¬íŠ¸ 8000ì´ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸
  ```bash
  # Windows
  netstat -ano | findstr :8000
  
  # Linux/Mac
  lsof -i :8000
  ```

**ë¬¸ì œ: vLLM ì„œë²„ OOM (Out of Memory)**
- í•´ê²°: `--gpu-memory-utilization` ê°’ ë‚®ì¶”ê¸° (0.8 â†’ 0.6)
- ë˜ëŠ” FastAPI ë¨¼ì € ì‹œì‘ í›„ vLLM ê¸°ë™

**ë¬¸ì œ: ë‹µë³€ ìƒì„±ì´ ì—¬ì „íˆ ëŠë¦¼**
- í™•ì¸: vLLM ì„œë²„ê°€ ì •ìƒ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸
  ```bash
  curl http://localhost:8400/v1/models
  ```
- í™•ì¸: FastAPIì—ì„œ vLLM client ì‚¬ìš©í•˜ëŠ”ì§€ ë¡œê·¸ í™•ì¸

**ë¬¸ì œ: KeyBERT/bge-m3 ì¤‘ë³µ ë¡œë“œë¨**
- í™•ì¸: GlobalBoot ë¡œê·¸ì—ì„œ "ì£¼ì… ì™„ë£Œ" ë©”ì‹œì§€ í™•ì¸
- í™•ì¸: ë‹¨ë… ì‹¤í–‰ì´ ì•„ë‹Œ API ì„œë²„ ì‹¤í–‰ì¸ì§€ í™•ì¸

---

## ğŸ“ˆ ì„±ëŠ¥ ê°œì„  ê²°ê³¼ (ì˜ˆìƒ)

### ë‹µë³€ ìƒì„± ì‹œê°„
- **ë³€ê²½ ì „**: 30ì´ˆ ì´ìƒ (ëª¨ë¸ ë¡œë“œ ì‹œê°„ í¬í•¨)
- **ë³€ê²½ í›„**: 8-12ì´ˆ (vLLM ì„œë²„ì—ì„œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ)
- **ê°œì„ **: 18-22ì´ˆ ë‹¨ì¶•

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (FastAPI í”„ë¡œì„¸ìŠ¤ ê¸°ì¤€)
- **KeyBERT**: 2íšŒ ë¡œë“œ â†’ 1íšŒ ë¡œë“œ (300MB ì ˆì•½)
- **bge-m3**: 2íšŒ ë¡œë“œ â†’ 1íšŒ ë¡œë“œ (1.5GB ì ˆì•½)
- **LLM ëª¨ë¸**: FastAPIì—ì„œ ì œê±° (26GB ì ˆì•½, vLLM ì„œë²„ë¡œ ì´ë™)
- **FastAPI í”„ë¡œì„¸ìŠ¤**: 28GB â†’ 500MB

### ì„œë²„ ê¸°ë™ ì‹œê°„
- **ë³€ê²½ ì „**: 3ê°œ í„°ë¯¸ë„ ì—´ê³  ìˆœì„œëŒ€ë¡œ ëª…ë ¹ì–´ ì…ë ¥ (ìˆ˜ë™ 3-5ë¶„)
- **ë³€ê²½ í›„**: python start_services.py 1íšŒ ì‹¤í–‰ (ìë™ 2-3ë¶„)
- **ê°œì„ **: ëª…ë ¹ì–´ 3ê°œ â†’ 1ê°œ

---

## ğŸ¯ ì¶”ê°€ ìµœì í™” ì˜µì…˜

### **1. ë¡œì»¬ ì„ë² ë”© ëª¨ë“œ** (Docker ì—†ì„ ë•Œ)

**ê¸°ë³¸**: Dockerë¡œ TEI ì„œë²„ ìë™ ì‹¤í–‰  
**ëŒ€ì•ˆ**: FastAPI í”„ë¡œì„¸ìŠ¤ ë‚´ë¶€ì—ì„œ SentenceTransformer ì§ì ‘ ë¡œë“œ

```bash
export USE_REMOTE_EMBEDDING=false
python start_services.py
```

**íš¨ê³¼**: Docker ë¶ˆí•„ìš”, ë‹¨ FastAPI ë©”ëª¨ë¦¬ +1.5GB

---

### **2. ë‹¤ì¤‘ vLLM ì¸ìŠ¤í„´ìŠ¤** (ë†’ì€ ë¶€í•˜ ì‹œ)

```bash
# vLLM ì„œë²„ 2ëŒ€ ì‹¤í–‰
python -m vllm.entrypoints.openai.api_server --port 8400 &
python -m vllm.entrypoints.openai.api_server --port 8401 &

# Nginxë¡œ ë¡œë“œ ë°¸ëŸ°ì‹±
upstream vllm_backend {
    server localhost:8400;
    server localhost:8401;
}
```

### **3. ëª¨ë‹ˆí„°ë§ ì¶”ê°€**
   - Prometheus + Grafana
   - ë‹µë³€ ìƒì„± ì‹œê°„ ì¶”ì 
   - ì„œë²„ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§

4. **Redis ìºì‹±**
   - Redis ìºì‹œ ì¶”ê°€
   - ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ ìºì‹±

---

## ğŸ“ ë¬¸ì˜ ë° ì§€ì›

**ë¬¸ì œ ë°œìƒ ì‹œ:**
1. ë¡œê·¸ íŒŒì¼ í™•ì¸ (`fastapi_server.log`, `vllm_server.log`)
2. GitHub Issues ë“±ë¡
3. ê°œë°œíŒ€ ë¬¸ì˜