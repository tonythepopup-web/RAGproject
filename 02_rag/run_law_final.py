# =============================================================
# run_rag_independent.py  (ì£¼ì„ ìƒì„¸íŒ)
# -------------------------------------------------------------
# ëª©ì :
#   - "ì´ë¯¸ ìƒì„±ëœ" ë¬¸ì„œë³„ ì¸ë±ìŠ¤(idx_singlelevel/idx_<ì¹´í…Œê³ ë¦¬>/)ë§Œ ì½ì–´ì„œ
#     ê²€ìƒ‰ â†’ (ì„ íƒ)LLM ë‹µë³€ â†’ ë¼ë²¨ë§ê¹Œì§€ ë‹¨ì¼ ìŠ¤í¬ë¦½íŠ¸ë¡œ ìˆ˜í–‰.
# íŠ¹ì§•:
#   - ì „ì²˜ë¦¬/ì¸ë±ìŠ¤ ìƒì„± ë‹¨ê³„ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ. (ì˜¤ì§ ë¡œë”©+ê²€ìƒ‰)
#   - ê²€ìƒ‰ ì ìˆ˜ëŠ” ì˜ë¯¸ë¡ (ì„ë² ë”©) + í‚¤ì›Œë“œ(IDF) ê°€ì¤‘ í•©ì˜ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹.
#   - LLMì€ VARCO-VISIONì„ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ í˜¸ì¶œ(ì´ë¯¸ì§€ ì…ë ¥ ì—†ìŒ).
#   - ì½”ë“œ/ì´ë¦„/ë¡œì§ì€ ì›ë³¸ê³¼ ë™ì¼í•˜ë©°, ì£¼ì„ë§Œ ìì„¸íˆ ë³´ê°•.
# ë””ë ‰í„°ë¦¬ ê¸°ëŒ€ êµ¬ì¡°(ì˜ˆ):
#   idx_singlelevel/
#     â”œâ”€ idx_ì „ì²´/
#     â”‚    â”œâ”€ chunks.json      # ì²­í¬ ë³¸ë¬¸/ë©”íƒ€/í‚¤ì›Œë“œ(enriched_text í¬í•¨)
#     â”‚    â”œâ”€ docs.txt         # (ì°¸ê³ ) ì›ë¬¸ ìš”ì•½/ê²½ë¡œ ë“± í…ìŠ¤íŠ¸
#     â”‚    â”œâ”€ vectors.npy      # ë¬¸ì„œ ì„ë² ë”©(ë¬¸ì„œ/ì²­í¬ ê¸°ì¤€)
#     â”‚    â””â”€ index.faiss      # FAISS ì¸ë±ìŠ¤
#     â”œâ”€ idx_ì‹í’ˆìœ„ìƒë²•/
#     â””â”€ ...
# ì‚¬ìš© íë¦„:
#   1) ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ â†’ LLM ë¡œë“œ(ì‹¤íŒ¨ ì‹œ ê²€ìƒ‰/ë¼ë²¨ë§ë§Œ) â†’ ì¸ë±ìŠ¤ ë¡œë“œ
#   2) ì‚¬ìš©ìê°€ ì§ˆì˜ ì…ë ¥ â†’ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ/ì„ íƒ â†’ ê²€ìƒ‰ Top-k ì‚°ì¶œ
#   3) (ì„ íƒ) LLMì— ìƒìœ„ 3ê°œ ì²­í¬ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±í•˜ì—¬ ë‹µë³€ ìƒì„±
#   4) CLIì—ì„œ good/bad ë³µìˆ˜ ì„ íƒ â†’ triplets JSONLë¡œ ë¼ë²¨ ì €ì¥
# =============================================================

import os, re, json, time, textwrap, math, sys
from datetime import datetime
from typing import List, Dict
from pathlib import Path

import numpy as np
import faiss
import torch

from sentence_transformers import SentenceTransformer
from keybert import KeyBERT
from transformers import AutoProcessor

# config.pyì—ì„œ vLLM client ê°€ì ¸ì˜¤ê¸°
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from config import get_vllm_varco_client

SCRIPT_DIR = Path(__file__).resolve().parent
EMBED_MODEL_NAME = "dragonkue/BGE-m3-ko"                       # ì„ë² ë”© ëª¨ë¸ ì´ë¦„
LLM_MODEL_ID     = "NCSOFT/VARCO-VISION-2.0-14B"               # LLM ëª¨ë¸ ID

# ë‚ ì§œë³„ ì¸ë±ìŠ¤ ë¡œë“œ (indexes/law/YYYY-MM-DD êµ¬ì¡°)
# ê³ ì •: ì‹¤ì œ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë¡œ ì„¤ì •
INDEX_DATE = "2025-11-11"  # ì¸ë±ìŠ¤ ìƒì„± ë‚ ì§œ (ê³ ì •)
PARENT_DIR       = str(SCRIPT_DIR.parent / "00_data" / "input" / "indexes" / "law" / INDEX_DATE)  # ì¸ë±ìŠ¤ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬
SAVE_LOG         = str(SCRIPT_DIR.parent / "00_data" / "output" / "logs" / "result.txt")  # (ì˜µì…˜) ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ íŒŒì¼ ê²½ë¡œ
TRIPLET_JSONL    = str(SCRIPT_DIR.parent / "00_data" / "output" / "training_data" / "triplets_group_bgem3.jsonl")  # ë¼ë²¨ ëˆ„ì  ì €ì¥ ê²½ë¡œ(JSONL)

# -------------------- ì „ì—­ ëª¨ë¸ --------------------
# GPU ë©”ëª¨ë¦¬ ìºì‹œ ë¹„ìš°ê¸°(íŠ¹íˆ ì¬ì‹¤í–‰ ì‹œ ì”ì—¬ ìºì‹œë¡œ ì¸í•œ OOM ë°©ì§€ìš©)
torch.cuda.empty_cache()

# KeyBERT, bge-m3: Noneìœ¼ë¡œ ì´ˆê¸°í™” (GlobalBootì—ì„œ ì£¼ì…ë˜ê±°ë‚˜, ë‹¨ë… ì‹¤í–‰ ì‹œ ë¡œë“œ)
# GlobalBoot ì‚¬ìš© ì‹œ ì´ ë³€ìˆ˜ë“¤ì€ GlobalBoot.__init__()ì—ì„œ ë®ì–´ì”Œì›Œì§
kw_model = None
embed_model = None

# ë‹¨ë… ì‹¤í–‰ í™•ì¸ í•¨ìˆ˜ (GlobalBootì—ì„œ í˜¸ì¶œë˜ì§€ ì•Šì„ ë•Œë§Œ ë¡œë“œ)
def _init_models_if_needed():
    """ë‹¨ë… ì‹¤í–‰ ëª¨ë“œì¼ ë•Œë§Œ ëª¨ë¸ ë¡œë“œ"""
    global kw_model, embed_model
    
    if kw_model is None:
        print("[INFO] KeyBERT ë¡œë“œ ì¤‘ (ë‹¨ë… ì‹¤í–‰ ëª¨ë“œ)...")
        kw_model = KeyBERT("paraphrase-multilingual-MiniLM-L12-v2")
    
    if embed_model is None:
        print("[INFO] BGE-m3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ (ë‹¨ë… ì‹¤í–‰ ëª¨ë“œ)...")
        from config import USE_REMOTE_EMBEDDING, EmbeddingModelWrapper
        
        if USE_REMOTE_EMBEDDING:
            print("   ì›ê²© ì„ë² ë”© ì„œë²„ ì‚¬ìš©")
            embed_model = EmbeddingModelWrapper(local_model=None, use_remote=True)
        else:
            print("   ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ")
            local_model = SentenceTransformer(EMBED_MODEL_NAME)
            embed_model = EmbeddingModelWrapper(local_model=local_model, use_remote=False)

# -------------------- ìœ í‹¸ (ì›ë³¸ê³¼ ë™ì¼) --------------------
# í•œêµ­ì–´ ì¡°ì‚¬ë¥¼ ë‹¨ìˆœ ì ‘ë¯¸ ì œê±°(í‚¤ì›Œë“œ ì •ê·œí™”).
#  - ëª©ì : "ì‹í’ˆ" vs "ì‹í’ˆì„" ê°™ì€ ë³€í˜•ì„ í†µì¼í•˜ì—¬ í‚¤ì›Œë“œ ì¼ì¹˜ìœ¨ í–¥ìƒ.
#  - ì£¼ì˜: í˜•íƒœì†Œ ë¶„ì„ì´ ì•„ë‹Œ ë‹¨ìˆœ ì ‘ë¯¸ ì œê±°ì´ë¯€ë¡œ ì¼ë¶€ ê³¼ë„ ì œê±°ë‚˜ ëˆ„ë½ ê°€ëŠ¥.
def remove_josa(word):
    for j in ['ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì—','ì˜','ì™€','ê³¼','ë„','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','í•œí…Œ','ë¶€í„°','ê¹Œì§€','ë§Œ','ë³´ë‹¤','ì²˜ëŸ¼','ì¡°ì°¨','ë§ˆì €']:
        if word.endswith(j):
            return word[:-len(j)]
    return word

# í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ
#  - ratioë¡œ í…ìŠ¤íŠ¸ ê¸¸ì´ ëŒ€ë¹„ ì¶”ì¶œ ê°œìˆ˜ ëŒ€ëµ ì¡°ì ˆ(ì•ˆì • ë²”ìœ„ min_k~max_k)
#  - ì˜ˆì™¸ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜(íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨ ë°©ì§€)
#  - ì¤‘ë³µ ì œê±°(set) + ì¡°ì‚¬ ì œê±° í›„ ë¦¬ìŠ¤íŠ¸í™”
def get_keywords(text, ratio=1, max_k=30, min_k=5):
    est_k = int(len(text) * ratio / 10)
    top_k = max(min_k, min(est_k, max_k))
    try:
        kws = kw_model.extract_keywords(text, top_n=top_k)
    except Exception:
        return []
    return list({remove_josa(k[0]) for k in kws if isinstance(k, (list, tuple)) and k and isinstance(k[0], str)})

# ì§ˆì˜ ë¬¸ìì—´ ì •ê·œí™”
#  - "ì œ 47" â†’ "47" (ìˆ«ìë§Œ ë‚¨ê¹€)
#  - "89ì¡° ì˜ 2" â†’ "89ì¡°ì˜2" (í‘œê¸° ì¼ê´€í™”)
#  - í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°
def preprocess_query(q):
    q = re.sub(r'ì œ\s*(\d+)', r'\1', q)
    q = re.sub(r'(\d+)ì¡°\s*ì˜\s*(\d+)', r'\1ì¡°ì˜\2', q)
    q = re.sub(r'[^\wê°€-í£\s]', '', q)
    return q.strip()

# ì†ŒìŠ¤ ë¬¸ìì—´(í‘œì œ/ë©”íƒ€) ì •ë¦¬
#  - (ì œní˜¸), (YYYYMMDD) íŒ¨í„´ ì œê±° â†’ ì‚¬ìš©ì í‘œì‹œìš© ê°„ì†Œí™”
#  - ë‹¤ì¤‘ ê³µë°± â†’ ë‹¨ì¼ ê³µë°±
def clean_source(src: str) -> str:
    src = re.sub(r'\(ì œ\d+í˜¸\)', '', src)
    src = re.sub(r'\(\d{8}\)', '', src)
    return re.sub(r'\s+', ' ', src).strip()


# -------------------- ì¹´í…Œê³ ë¦¬/ë§¤í•‘ (ì›ë³¸ê³¼ ë™ì¼) --------------------
# ê²€ìƒ‰ ë²”ìœ„ë¥¼ ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬, ì‚¬ìš©ì ì„ íƒ ë˜ëŠ” ìë™ ì¶”ì²œì— í™œìš©
desired_categories = [
    "ê°€ì¶•ì „ì—¼ë³‘ ì˜ˆë°©ë²•","ê±´ê°•ê¸°ëŠ¥ì‹í’ˆì— ê´€í•œ ë²•ë¥ ","ë†ì•½ê´€ë¦¬ë²•",
    "ë¨¹ëŠ”ë¬¼ê´€ë¦¬ë²•","ì‚¬ë£Œê´€ë¦¬ë²•","ìˆ˜ì…ì‹í’ˆì•ˆì „ê´€ë¦¬","ì‹í’ˆìœ„ìƒë²•",
    "ì‹í’ˆã†ì˜ì•½í’ˆë¶„ì•¼ ì‹œí—˜ã†ê²€ì‚¬","ì¶•ì‚°ë¬¼ ìœ„ìƒê´€ë¦¬ë²•","í•œêµ­ì‹í’ˆì•ˆì „ê´€ë¦¬ì¸ì¦ì›ì˜ ì„¤ë¦½ ë° ìš´ì˜ì— ê´€í•œ ë²•ë¥ "
]
# ìˆ«ì ì„ íƒ(0=ì „ì²´)ê³¼ í•œê¸€ ì´ë¦„ ì…ë ¥ì„ ëª¨ë‘ í—ˆìš©í•˜ê¸° ìœ„í•œ ë§¤í•‘ í…Œì´ë¸”
mapping = {str(i): cat for i, cat in enumerate(["ì „ì²´"] + desired_categories)}

# ì¹´í…Œê³ ë¦¬ëª… ë¬¸ì¥ ì„ë² ë”© (ì£¼ì˜: classify_categoryì—ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
# ì‹¤ì œë¡œëŠ” RAG ê²€ìƒ‰ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ (186ë²ˆ ë¼ì¸ ì°¸ê³ )
# category_embeddings = embed_model.encode(
#     desired_categories, normalize_embeddings=True, convert_to_tensor=False
# )

# ì‚¬ìš©ì ì…ë ¥ì„ ìˆ«ì/ë¬¸ì ì¡°í•© ëª¨ë‘ ì¸ì‹(ì˜ˆ: "1", "1ì‹í’ˆìœ„ìƒë²•", "ì‹í’ˆìœ„ìƒë²•")
def parse_category_input(inp):
    inp = inp.strip()
    if inp in mapping:
        return mapping[inp]
    m = re.match(r"(\d+)(.+)", inp)
    if m and m.group(1) in mapping:
        return mapping[m.group(1)]
    if inp in mapping.values():
        return inp
    return None
#ìˆ˜ì •

def parse_multi_category_input(inp: str) -> List[str]:
    """
    ì‰¼í‘œ/ê³µë°±ìœ¼ë¡œ ì—¬ëŸ¬ ê°œ ì…ë ¥ ì§€ì›.
    - '0' ë˜ëŠ” 'ì „ì²´'ê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ['ì „ì²´']ë§Œ ë°˜í™˜.
    - ì¤‘ë³µ ì œê±°, ì…ë ¥ ìˆœì„œ ìœ ì§€.
    """
    if not inp:
        return []
    tokens = [t for t in re.split(r'[,\s]+', inp.strip()) if t]
    if any(t == '0' or t == 'ì „ì²´' for t in tokens):
        return ['ì „ì²´']

    seen, out = set(), []
    for t in tokens:
        cat = parse_category_input(t)  # ê¸°ì¡´ ë‹¨ì¼ íŒŒì„œ ì¬ì‚¬ìš©
        if cat and cat not in seen:
            seen.add(cat)
            out.append(cat)
    return out

# def classify_category(query: str, sem_threshold: float = 0.4) -> List[str]:
#     q_vec = embed_model.encode([query], normalize_embeddings=True)[0]
#     sims = np.dot(category_embeddings, q_vec)
#     idx = np.where(sims >= sem_threshold)[0]
#     if idx.size == 0:
#         return []
#     idx = idx[np.argsort(sims[idx])[::-1]][:2]

#     return [desired_categories[i] for i in idx]

def classify_category(query: str, sem_threshold: float = 0.0) -> List[str]:
    """
    ê¸°ì¡´ ì„ë² ë”© ê¸°ë°˜ ì¶”ì²œ ëŒ€ì‹ ,
    ì‹¤ì œ RAG ê²€ìƒ‰ ì ìˆ˜(retrieve_docs ê²°ê³¼ score)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒìœ„ 2ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì²œí•œë‹¤.
    """
    # init_rag_from_saved()ì—ì„œ ë§Œë“  cat_indicesë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    indices = globals().get("cat_indices")
    if not isinstance(indices, dict):
        return []

    uq = preprocess_query(query)
    scored: list[tuple[str, float]] = []

    for cat, cfg in indices.items():
        if cat == "ì „ì²´":
            continue  # ì „ì²´ ì¸ë±ìŠ¤ëŠ” routing ëŒ€ìƒì—ì„œ ì œì™¸

        try:
            results = retrieve_docs(
                uq,
                cfg["model"], cfg["index"], cfg["docs"], cfg["chunks"], cfg["IDF"],
                top_k=3,        # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ top-3 ì²­í¬ ê²€ìƒ‰
            )
            best_score = max((r.get("score", 0.0) for r in results), default=0.0)
        except Exception:
            best_score = 0.0

        if best_score > 0.0:
            scored.append((cat, best_score))

    # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ, ë™ì ì´ë©´ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœ
    scored.sort(key=lambda x: (-x[1], x[0]))
    return [c for c, _ in scored[:2]]


# -------------------- ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë” (ì¸ë±ìŠ¤ ìƒì„± X, ì½ê¸°ë§Œ) --------------------
# ì§€ì • ì¹´í…Œê³ ë¦¬ì˜ ë””ìŠ¤í¬ ì¸ë±ìŠ¤ ë¬¶ìŒ(chunks/docs/vectors/index) ë¡œë“œ
#  - íŒŒì¼ ì¡´ì¬ì„± ê²€ì‚¬ í›„, FAISS ì¸ë±ìŠ¤ ì½ê¸° + ì²­í¬ ë¡œë“œ
#  - enriched_textëŠ” ê²€ìƒ‰/í”„ë¡¬í”„íŠ¸ì— í™œìš©ë˜ëŠ” ë³¸ë¬¸ í•„ë“œ
#  - compute_idf(chunks)ë¡œ ì¹´í…Œê³ ë¦¬ë³„ IDF ì‚¬ì „ ì¤€ë¹„(í‚¤ì›Œë“œ ê°€ì¤‘)
def load_saved_category(cat: str, parent_dir=PARENT_DIR) -> Dict:
    save_dir = os.path.join(parent_dir, f"idx_{cat}")
    jp = os.path.join(save_dir, "chunks.json")
    dp = os.path.join(save_dir, "docs.txt")
    vp = os.path.join(save_dir, "vectors.npy")
    ip = os.path.join(save_dir, "index.faiss")

    for p in (jp, dp, vp, ip):
        if not os.path.exists(p):
            raise FileNotFoundError(f"[{cat}] ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {p}")

    index = faiss.read_index(ip)
    with open(jp, "r", encoding="utf-8") as f:
        chunks = json.load(f)
    docs = [c["enriched_text"] for c in chunks]
    return {"model": embed_model, "index": index, "chunks": chunks, "docs": docs, "IDF": compute_idf(chunks)}

# ëª¨ë“  ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ì¸ë±ìŠ¤ ì‹œë„ ë¡œë“œ(ì—†ìœ¼ë©´ ê±´ë„ˆëœ€) + "ì „ì²´"ëŠ” í•„ìˆ˜ ë¡œë“œ
def init_rag_from_saved(parent_dir: str) -> Dict[str, Dict]:
    cat_indices = {}
    # ê°œë³„ ì¹´í…Œê³ ë¦¬ ë¡œë“œ(ì—†ìœ¼ë©´ passë¡œ ë¬´ì‹œ)
    for cat in desired_categories:
        try:
            cat_indices[cat] = load_saved_category(cat, parent_dir)
        except FileNotFoundError:
            pass
    # ì „ì²´(í†µí•© ì¸ë±ìŠ¤)ëŠ” ë°˜ë“œì‹œ ì¡´ì¬í•´ì•¼ ìœ íš¨í•˜ê²Œ ë™ì‘
    cat_indices["ì „ì²´"] = load_saved_category("ì „ì²´", parent_dir)
    return cat_indices

# -------------------- IDF (ì›ë³¸ê³¼ ë™ì¼) --------------------
# ì²­í¬ì— ë¯¸ë¦¬ ì €ì¥ëœ 'keywords' í•„ë“œì˜ ë¬¸ì„œë¹ˆë„(DF)ë¡œ IDF ê°€ì¤‘ì¹˜ ì‚°ì¶œ
#  - IDF = log((N+1)/(df+1)) + 1 : 0 ë¶„ëª¨/ë¶„ìë¥¼ í”¼í•˜ê³  ê³¼ë„í•œ ê°’ ë°©ì§€
#  - í‚¤ì›Œë“œ êµì§‘í•©ì— ëŒ€í•´ í•©ì‚°í•˜ì—¬ ì¿¼ë¦¬-ì²­í¬ í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°ì— ì‚¬ìš©
def compute_idf(chunks):
    N = len(chunks)
    df = {}
    for ch in chunks:
        for kw in set(ch['keywords']):
            df[kw] = df.get(kw, 0) + 1
    return {kw: math.log((N + 1) / (cnt + 1)) + 1 for kw, cnt in df.items()}

# -------------------- ê²€ìƒ‰: í•˜ì´ë¸Œë¦¬ë“œ(è¯­ä¹‰ + í‚¤ì›Œë“œ-IDF), ì›ë³¸ê³¼ ë™ì¼ --------------------
# ì ˆì°¨:
#  1) ì˜ë¯¸ ì ìˆ˜: ì¿¼ë¦¬ ì„ë² ë”© qv â†’ FAISSë¡œ ì „ì²´ í›„ë³´ ê²€ìƒ‰(ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë²”ìœ„ì— ë§ì¶° clip)
#  2) í‚¤ì›Œë“œ ì ìˆ˜: ì¿¼ë¦¬ í‚¤ì›Œë“œ vs ê° ì²­í¬ í‚¤ì›Œë“œ êµì§‘í•©ì˜ IDF í•©
#  3) ë‘ ì ìˆ˜ ê°€ì¤‘í•©( alpha*semantic + (1-alpha)*keyword ) í›„ ë‚´ë¦¼ì°¨ìˆœ Top-k
# ë°˜í™˜: ìƒìœ„ ì²­í¬(dict) ë¦¬ìŠ¤íŠ¸
def retrieve_docs(query, model, index, docs, chunks, IDF, alpha=0.9, top_k=5):
    qv = model.encode([query], normalize_embeddings=True)[0]
    dists, I = index.search(np.array([qv]), len(docs))  # ëª¨ë“  ë¬¸ì„œ ëŒ€ìƒ ê²€ìƒ‰ í›„ ì •ë ¬ ì¸ë±ìŠ¤ íšë“
    dists, I = dists[0], I[0]

    # ì˜ë¯¸ ì ìˆ˜: [0,1] ë²”ìœ„ë¡œ ì•ˆì „í•˜ê²Œ í´ë¦¬í•‘
    sem = np.clip(dists, 0, 1)

    # í‚¤ì›Œë“œ ì ìˆ˜: ì¿¼ë¦¬â†’í‚¤ì›Œë“œ ì¶”ì¶œ â†’ ê° í›„ë³´ ì²­í¬ì˜ í‚¤ì›Œë“œì™€ êµì§‘í•© IDF í•©
    qk = set(get_keywords(query))
    ks = np.array([sum(IDF.get(kw, 1.0) for kw in (qk & set(chunks[i]['keywords']))) for i in I], dtype=np.float32)
    if ks.max() > 0:
        ks /= (ks.max() + 1e-6)  # 0-1 ì •ê·œí™”(ë¶„ëª¨ 0 ë°©ì§€ìš© epsilon)

    # í•˜ì´ë¸Œë¦¬ë“œ ìµœì¢… ì ìˆ˜ ë° Top-k ì¸ë±ìŠ¤ ì„ íƒ
    scores = alpha * sem + (1 - alpha) * ks
    top_indices = I[np.argsort(scores)[::-1][:top_k]]
    
    # ì ìˆ˜ë¥¼ ì²­í¬ì— ì¶”ê°€í•˜ì—¬ ë°˜í™˜
    results = []
    for idx in top_indices:
        chunk = chunks[idx].copy()
        chunk['score'] = round(float(scores[np.where(I == idx)[0][0]]), 2)  # í•´ë‹¹ ì²­í¬ì˜ ì ìˆ˜ ì¶”ê°€
        
        # textë¥¼ enriched_textë¡œ êµì²´ (ë²•ë¥ ì€ enriched_text ì‚¬ìš©)
        if 'enriched_text' in chunk:
            chunk['text'] = chunk['enriched_text']
        
        results.append(chunk)
    return results

# -------------------- í”„ë¡¬í”„íŠ¸ (ì›ë³¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ) --------------------
# ìƒìœ„ N(ê¸°ë³¸ 3)ê°œ ì²­í¬ë¥¼ ChatML í¬ë§·ì˜ ì»¨í…ìŠ¤íŠ¸ ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±
#  - ë³¸ë¬¸ì€ ê°€ë…ì„±ì„ ìœ„í•´ wrap_widthë¡œ ì¤„ë°”ê¿ˆ
#  - system ì§€ì¹¨ì€ "ì˜¤ì§ ì£¼ì–´ì§„ ì¡°ë¬¸ìœ¼ë¡œë§Œ ë‹µë³€"í•˜ë„ë¡ í•œì •
def build_chatml_prompt(question: str, results: list, max_blocks: int = 3, wrap_width: int = 80) -> str:
    context_blocks = []
    for i, chunk in enumerate(results[:max_blocks], start=1):
        title = clean_source(chunk["source"])  # ì†ŒìŠ¤ í‘œì œ ì •ë¦¬
        body = chunk["text"]
        wrapped_body = "\n".join(textwrap.wrap(body, width=wrap_width))
        context_blocks.append(f"{i}. {title}\n{wrapped_body}")
    context = "\n\n".join(context_blocks)

    prompt = f"""<|im_start|>system
ë‹¹ì‹ ì€ í•œêµ­ ë²•ë ¹ì— ì •í†µí•œ ì „ë¬¸ ë²•ë¥  ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ì—ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë²•ë ¹ì—ì„œ ë°œì·Œí•œ ì¡°ë¬¸ë“¤ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì„¸ ê°€ì§€ì…ë‹ˆë‹¤:
1. ì¡°ë¬¸ë“¤ì„ ì½ê³  ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ” ë‚´ìš©ë§Œ ê³¨ë¼ í•µì‹¬ì„ ìš”ì•½í•˜ì„¸ìš”. 
2. ì˜¤ì§ ì£¼ì–´ì§„ ì¡°ë¬¸ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ì‹ ë¢°ë„ ë†’ì€ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
3. ë§Œì•½ ëª¨ë“  ì¡°ë¬¸ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ë‹¤ë©´, ê´€ë ¨ëœ ì¡°ë¬¸ì´ ì—†ìŒì„ ë°íˆê³  ë‹µë³€ì„ ìœ ë³´í•˜ì„¸ìš”.
[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
1. ìš”ì•½:
(ì—¬ê¸°ì— í•µì‹¬ ìš”ì•½)
2. ë‹µë³€:
(ì—¬ê¸°ì— ë‹µë³€ ë‚´ìš© â€” í•´ë‹¹ ì¡°ë¬¸ ë²ˆí˜¸ ì–¸ê¸‰ í¬í•¨)
<|im_end|>
<|im_start|>user
[ì‚¬ìš©ì ì§ˆë¬¸]
{question}
[ì¡°ë¬¸ ë‚´ìš©]
{context}
<|im_end|>
<|im_start|>assistant
""".strip()
    return prompt

# -------------------- LLM (vLLM ì„œë²„ ì‚¬ìš©) --------------------
def load_llm(model_id: str = LLM_MODEL_ID):
    """
    vLLM serving framework í™œìš©ì„ ìœ„í•´ ìˆ˜ì •ë¨
    
    ê¸°ì¡´ê³¼ ë‹¬ë¦¬ ëª¨ë¸ì„ ì§ì ‘ ë¡œë“œí•˜ì§€ ì•Šê³  í”„ë¡œì„¸ì„œë§Œ ë¡œë“œ
    ì‹¤ì œ ëª¨ë¸ì€ vLLM ì„œë²„ì—ì„œ ì„œë¹™ë¨
    """
    processor = AutoProcessor.from_pretrained(model_id)
    return None, processor

# Chat í…œí”Œë¦¿ ì ìš© â†’ vLLM ì„œë²„ í˜¸ì¶œ â†’ ì‘ë‹µ ë°˜í™˜
def generate_llm_response(model, processor, conversation, max_new_tokens=1024):
    """
    vLLM serving framework í™œìš©ì„ ìœ„í•´ ìˆ˜ì •ë¨
    
    ëª¨ë¸ì— ì§ì ‘ ì…ë ¥í•˜ì§€ ì•Šê³  vLLM clientë¡œ localhost:8400/v1ì— ìš”ì²­
    """
    # ChatML í…œí”Œë¦¿ ì ìš© (í† í¬ë‚˜ì´ì¦ˆëŠ” ì„œë²„ì—ì„œ ì²˜ë¦¬)
    rendered_prompt = processor.apply_chat_template(
        conversation, add_generation_prompt=True, tokenize=False
    )

    # í† í° ê¸¸ì´ ê³„ì‚° (max_tokens ì„¤ì •ìš©)
    input_len = len(processor.tokenizer(rendered_prompt)["input_ids"])
    
    # max_tokens ì•ˆì „í•˜ê²Œ ê³„ì‚°
    MAX_CONTEXT_LENGTH = 4096
    RESERVED_OUTPUT_TOKENS = 1024
    MIN_OUTPUT_TOKENS = 100
    
    available_tokens = MAX_CONTEXT_LENGTH - input_len
    max_tokens = min(available_tokens, RESERVED_OUTPUT_TOKENS)
    
    if max_tokens < MIN_OUTPUT_TOKENS:
        # ì…ë ¥ì´ ë„ˆë¬´ ê¸¸ì–´ì„œ ì¶œë ¥ ê³µê°„ì´ ë¶€ì¡±í•œ ê²½ìš°
        print(f"âš ï¸ ê²½ê³ : ì…ë ¥ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({input_len} tokens). ìµœì†Œ ì¶œë ¥ í† í° ë³´ì¥ ë¶ˆê°€.")
        max_tokens = max(1, available_tokens)  # ìµœì†Œí•œ 1 í† í°ì€ ë³´ì¥

    # vLLM ì„œë²„ì— ìš”ì²­
    client = get_vllm_varco_client()
    start = time.time()
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL_ID,
            messages=conversation,
            max_tokens=max_tokens
        )
        elapsed = time.time() - start
        output_text = response.choices[0].message.content
        return {"rendered_prompt": rendered_prompt, "output": output_text.strip(), "elapsed": elapsed}
    except Exception as e:
        elapsed = time.time() - start
        print(f"âŒ vLLM ì„œë²„ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return {"rendered_prompt": rendered_prompt, "output": f"[ì˜¤ë¥˜] vLLM ì„œë²„ ì‘ë‹µ ì‹¤íŒ¨: {str(e)}", "elapsed": elapsed}

# -------------------- Triplet ë¼ë²¨ë§ (ì›ë³¸ê³¼ ë™ì¼) --------------------
# (ì§ˆë¬¸, positives[], negatives[]) í•œ ê±´ì„ JSONLë¡œ í•œ ì¤„ ì €ì¥
#  - ê°œí–‰/íƒ­ ë“± ê³µë°±ë¥˜ëŠ” ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì •ê·œí™”í•˜ì—¬ ì €ì¥
def save_group_jsonl(query: str,
                     positives: List[str],
                     negatives: List[str],
                     pos_sources: List[str] | None = None,
                     neg_sources: List[str] | None = None,
                     extra_meta: dict | None = None,
                     out_path: str = TRIPLET_JSONL):
    def _clean(s: str) -> str:
        s = s.replace("\t", " ").replace("\r", " ").replace("\n", " ")
        return re.sub(r"\s+", " ", s).strip()

    rec = {
        "query": _clean(query),
        "positives": [_clean(p) for p in positives if p and p.strip()],
        "negatives": [_clean(n) for n in negatives if n and n.strip()],
        "meta": {"timestamp": datetime.now().isoformat(timespec="seconds")}
    }
    if pos_sources: rec["meta"]["pos_sources"] = [_clean(x) for x in pos_sources]
    if neg_sources: rec["meta"]["neg_sources"] = [_clean(x) for x in neg_sources]
    if extra_meta:  rec["meta"].update(extra_meta)

    # í´ë” ìë™ ìƒì„±
    Path(out_path).parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")

# CLIì—ì„œ í›„ë³´ë“¤ì„ ë³´ê³  good/badë¥¼ ë³µìˆ˜ ì„ íƒ â†’ JSONL ì €ì¥
#  - ì…ë ¥ ì˜ˆ: good: "1,3" / bad: "2,5"
#  - good/bad ì¤‘ë³µ ì„ íƒ ì‹œ ê²¹ì¹˜ëŠ” í•­ëª©ì€ ì œì™¸ ì²˜ë¦¬
def interactive_label_group(question: str,
                            candidates: List[dict],
                            llm_used_n: int = 3) -> bool:
    if not candidates:
        print("âš ï¸ ë¼ë²¨ë§í•  í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False

    print("\n===== [ë¼ë²¨ë§] ë‹¤ì¤‘ good/bad ì„ íƒ =====")
    print(f"[ì§ˆë¬¸]\n{question}\n")
    print("[í›„ë³´ ëª©ë¡] (ë²ˆí˜¸ ì°¸ê³ )")
    for i, ch in enumerate(candidates, start=1):
        title = clean_source(ch["source"])
        body = ch["enriched_text"].strip()
        preview = body  # ë¯¸ë¦¬ë³´ê¸°(ì¤„ì„ ì²˜ë¦¬ ì—†ìŒ)
        print(f"{i}) {title}\n   {preview}\n")

    print("ì˜ˆ) good: 1,3   bad: 2,5   (ë¹„ìš°ë©´ ê±´ë„ˆëœ€)")
    good_in = input("good(ì •ë‹µ) ë²ˆí˜¸(ì½¤ë§ˆêµ¬ë¶„): ").strip()
    bad_in  = input("bad(ì˜¤ë‹µ)  ë²ˆí˜¸(ì½¤ë§ˆêµ¬ë¶„): ").strip()

    def parse_indices(s: str, N: int) -> List[int]:
        if not s: return []
        vals = []
        for tok in s.split(","):
            tok = tok.strip()
            if tok.isdigit():
                v = int(tok)
                if 1 <= v <= N:
                    vals.append(v-1)
        return sorted(set(vals))

    pos_idx = parse_indices(good_in, len(candidates))
    neg_idx = parse_indices(bad_in, len(candidates))
    if not pos_idx and not neg_idx:
        print("â¡ï¸ ì…ë ¥ì´ ì—†ì–´ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False

    overlap = set(pos_idx) & set(neg_idx)
    if overlap:
        print(f"âš ï¸ ê²¹ì¹˜ëŠ” ë²ˆí˜¸ ì œì™¸: {[i+1 for i in overlap]}")
        pos_idx = [i for i in pos_idx if i not in overlap]
        neg_idx = [i for i in neg_idx if i not in overlap]

    positives = [candidates[i]["enriched_text"] for i in pos_idx]
    negatives = [candidates[i]["enriched_text"] for i in neg_idx]
    pos_srcs  = [clean_source(candidates[i]["source"]) for i in pos_idx]
    neg_srcs  = [clean_source(candidates[i]["source"]) for i in neg_idx]

    if not positives and not negatives:
        print("âš ï¸ ìœ íš¨í•œ ì„ íƒì´ ì—†ì–´ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False

    meta = {
        "retrieved_topk": len(candidates),
        "llm_used_topn": llm_used_n,
        "pos_indices_1based": [i+1 for i in pos_idx],
        "neg_indices_1based": [i+1 for i in neg_idx]
    }
    save_group_jsonl(question, positives, negatives, pos_srcs, neg_srcs, meta)
    print(f"âœ… ê·¸ë£¹ ì €ì¥ ì™„ë£Œ â†’ {TRIPLET_JSONL}")
    return True

# -------------------- ë©”ì¸ (ì¸ë±ìŠ¤ëŠ” ì½ê¸°ë§Œ) --------------------
# ì‹¤í–‰ ì§„ì…ì : LLM ë¡œë“œ â†’ ì¸ë±ìŠ¤ ë¡œë“œ â†’ ì§ˆì˜ ë£¨í”„(ê²€ìƒ‰/ë‹µë³€/ë¼ë²¨ë§)
def main():
    # 0) ë‹¨ë… ì‹¤í–‰ ëª¨ë“œì¼ ë•Œ ëª¨ë¸ ë¡œë“œ
    _init_models_if_needed()
    
    # 1) LLM ë¡œë“œ (ì›ë³¸ ë™ì¼)
    try:
        model_llm, processor = load_llm(LLM_MODEL_ID)
        USE_LLM = True
        print("[INFO] LLM ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ë©”ëª¨ë¦¬/ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ë“±ìœ¼ë¡œ ì‹¤íŒ¨ ê°€ëŠ¥
        print(f"[ê²½ê³ ] LLM ë¡œë“œ ì‹¤íŒ¨ â†’ ê²€ìƒ‰/ë¼ë²¨ë§ë§Œ ì‚¬ìš©: {e}")
        model_llm = processor = None
        USE_LLM = False

    # 2) ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ (ì¸ë±ìŠ¤ ìƒì„±/ì „ì²˜ë¦¬ ì—†ìŒ)
    parent_dir = PARENT_DIR
    cat_indices = init_rag_from_saved(parent_dir)
    #ìˆ˜ì •
    globals()["cat_indices"] = cat_indices
    #
    # ê°„ë‹¨ ì±„íŒ… ë¡œê·¸ íŒŒì¼(ì„¸ì…˜ ì‹œì‘ í—¤ë”ë§Œ ê¸°ë¡)
    log_path = "chat_log.txt"
    with open(log_path, "a", encoding="utf-8") as log:
        log.write(f"\n\n===== VARCO-VISION + RAG ì„¸ì…˜ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} =====\n")

    # 3) ëŒ€í™”í˜• ë£¨í”„: exit ì…ë ¥ ì „ê¹Œì§€ ë°˜ë³µ
    while True:
        q = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit) >> ").strip()
        if not q:
            continue
        if q.lower() == "exit":
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ì¹´í…Œê³ ë¦¬ ëª©ë¡ í‘œì‹œ(ìˆ«ì ì„ íƒ ìš©ì´)
        print("\n=== ì „ì²´ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ===")
        for i in range(0, len(desired_categories)+1):
            print(f"{i}) {mapping[str(i)]}", end="    ")
            if (i % 6) == 5:
                print()
        print("\n")

        # ì§ˆì˜ ê¸°ë°˜ ì¶”ì²œ ì¹´í…Œê³ ë¦¬ ë³´ì—¬ì£¼ê¸°(í‚¤ì›Œë“œ/ì˜ë¯¸ ê¸°ë°˜)
        candidates_cat = classify_category(q)
        print("=== ì¶”ì²œ ì¹´í…Œê³ ë¦¬ ===")
        for i, cat in enumerate(desired_categories, start=1):
            if cat in candidates_cat:
                print(f"{i}) {cat}")
        print("0) ì „ì²´")

        # ì§ì ‘ ì„ íƒ(ë¯¸ì…ë ¥ ì‹œ ì¶”ì²œ 1ìˆœìœ„ ë˜ëŠ” ì „ì²´ë¡œ ëŒ€ì²´)
        
        # choice = input("ë²ˆí˜¸/ì´ë¦„ ì…ë ¥(ë¯¸ì…ë ¥=ì¶”ì²œâ†’ì „ì²´/ì²«ë²ˆì§¸) >> ").strip()
        # sel = parse_category_input(choice) if choice else None
        # if not sel:
        #     sel = candidates_cat[0] if candidates_cat else "ì „ì²´"

        # if sel not in cat_indices:
        #     print(f"[ê²½ê³ ] '{sel}' ì¸ë±ìŠ¤ê°€ ì—†ì–´ 'ì „ì²´'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        #     sel = "ì „ì²´"
# ìˆ˜ì •
        choice = input("ë²ˆí˜¸/ì´ë¦„ (ì‰¼í‘œë¡œ ì—¬ëŸ¬ ê°œ, ë¯¸ì…ë ¥=ì¶”ì²œ ìƒìœ„/ì „ì²´) >> ").strip()

        if choice:
            sel_list = parse_multi_category_input(choice)
        else:
            # ë¯¸ì…ë ¥: ì¶”ì²œì´ ìˆìœ¼ë©´ ìµœëŒ€ 2ê°œ, ì—†ìœ¼ë©´ 'ì „ì²´'
            sel_list = candidates_cat[:2] if candidates_cat else ['ì „ì²´']

        # ë¡œë“œëœ ì¸ë±ìŠ¤ë§Œ ìœ ì§€
        sel_list = [c for c in sel_list if c in cat_indices]
        if not sel_list:
            sel_list = ['ì „ì²´']

        print("\n=== ì„ íƒëœ ì¹´í…Œê³ ë¦¬ ===")
        print(", ".join(sel_list))
#
        # --- ê²€ìƒ‰(í•˜ì´ë¸Œë¦¬ë“œ) ---
        # cfg = cat_indices[sel]
        # uq = preprocess_query(q)  # ìˆ«ì/ë¬¸ì¥ ê·œê²©í™”
        # results_top5 = retrieve_docs(uq, cfg["model"], cfg["index"], cfg["docs"], cfg["chunks"], cfg["IDF"], top_k=5)
        # results_for_prompt = results_top5[:3]

        # print(f"\n== {sel} ê²€ìƒ‰ ê²°ê³¼ ==")
        # for r in results_for_prompt:
        #     print(r["source"])  # ì‚¬ìš©ìì—ê²Œ ì¶œì²˜ë§Œ í‘œì‹œ(ë³¸ë¬¸ ë¯¸í‘œì‹œ)

# ìˆ˜ì •
        uq = preprocess_query(q)

        def _dedup_by_source(rows: List[dict]) -> List[dict]:
            """source ê¸°ì¤€ ì¤‘ë³µ ì œê±°(ì²« ë“±ì¥ ìœ ì§€). í•„ìš” ì—†ìœ¼ë©´ ì œê±° ê°€ëŠ¥."""
            seen, out = set(), []
            for r in rows:
                key = r.get("source")
                if key in seen:
                    continue
                seen.add(key)
                out.append(r)
            return out

        if sel_list == ['ì „ì²´']:
            cfg = cat_indices['ì „ì²´']
            results_top5 = retrieve_docs(
                uq, cfg["model"], cfg["index"], cfg["docs"], cfg["chunks"], cfg["IDF"],
                top_k=5
            )
        else:
            aggregated = []
            per_cat_k = 3  # ì¹´í…Œê³ ë¦¬ë³„ ëª‡ ê°œì”© ë½‘ì„ì§€
            for sel in sel_list:
                cfg = cat_indices[sel]
                part = retrieve_docs(
                    uq, cfg["model"], cfg["index"], cfg["docs"], cfg["chunks"], cfg["IDF"],
                    top_k=per_cat_k
                )
                for r in part:
                    r["category"] = sel  # ë””ë²„ê¹…/ë¡œê·¸ìš© íƒœê·¸
                aggregated.extend(part)

            aggregated = _dedup_by_source(aggregated)
            aggregated.sort(key=lambda x: x.get("score", 0.0), reverse=True)
            results_top5 = aggregated[:5]

        results_for_prompt = results_top5[:3]

        print(f"\n== ê²€ìƒ‰ ê²°ê³¼ (ì„ íƒ: {', '.join(sel_list)}) ==")
        for r in results_for_prompt:
            cat_tag = f"[{r.get('category')}]" if r.get('category') else ""
            print(f"{cat_tag} {r['source']}")

# ìˆ˜ì •

        # --- LLM ë‹µë³€ (ì›ë³¸ê³¼ ë™ì¼ í”„ë¡¬í”„íŠ¸ / í…ìŠ¤íŠ¸ë§Œ) ---
        if USE_LLM:
            chatml_prompt = build_chatml_prompt(q, results_for_prompt, max_blocks=3, wrap_width=80)
            conversation = [{"role": "user", "content": [{"type": "text", "text": chatml_prompt}]}]
            gen = generate_llm_response(model_llm, processor, conversation, max_new_tokens=1024)

            print(f"\nâœ… LLM ì‘ë‹µ (â± {gen['elapsed']:.2f}s)\n")
            print(gen["output"])  # ëª¨ë¸ ì¶œë ¥ ê·¸ëŒ€ë¡œ í‘œì‹œ

            # ì„¸ë¶€ ë¡œê·¸(í”„ë¡¬í”„íŠ¸ ì›ë¬¸/ì‘ë‹µ/ì†Œìš”ì‹œê°„)
            with open(log_path, "a", encoding="utf-8") as log:
                log.write(f"\nğŸ‘¤ ì§ˆë¬¸: {q}\n")
                # log.write(f"ğŸ“‚ ì„ íƒ ì¹´í…Œê³ ë¦¬: {sel}\n")
                # ìˆ˜ì •
                log.write(f"ğŸ“‚ ì„ íƒ ì¹´í…Œê³ ë¦¬: {', '.join(sel_list)}\n")
                #
                for r in results_for_prompt:
                    log.write(f" - {r['source']}\n")
                log.write("\n--- Rendered Prompt ---\n")
                log.write(gen["rendered_prompt"] + "\n")
                log.write(f"\nğŸ¤– VARCO ì‘ë‹µ:\n{gen['output']}\n")
                log.write(f"â± ì†Œìš”ì‹œê°„: {gen['elapsed']:.2f}ì´ˆ\n")
        else:
            print("\n[ì•ˆë‚´] LLM ë¹„í™œì„±/ë¡œë”© ì‹¤íŒ¨ë¡œ ë‹µë³€ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        # --- ë¼ë²¨ë§ (ì›ë³¸ ë¡œì§) ---
        input("\n(ì—”í„°ë¥¼ ëˆ„ë¥´ë©´ ë¼ë²¨ë§ ë‹¨ê³„ë¡œ ì´ë™í•©ë‹ˆë‹¤) ")
        _ = interactive_label_group(q, results_top5, llm_used_n=len(results_for_prompt))

if __name__ == "__main__":
    main()
