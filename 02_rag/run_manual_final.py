# -------------------------------------------------------------
# ëª©ì :
#   - "ì´ë¯¸ ìƒì„±ëœ" ë¬¸ì„œë³„ ì¸ë±ìŠ¤(idx_singlelevel/idx_<ì¹´í…Œê³ ë¦¬>/)ë§Œ ì½ì–´ì„œ
#     ê²€ìƒ‰ â†’ (ì„ íƒ)LLM ë‹µë³€ â†’ ë¼ë²¨ë§ê¹Œì§€ ë‹¨ì¼ ìŠ¤í¬ë¦½íŠ¸ë¡œ ìˆ˜í–‰.
# íŠ¹ì§•:
#   - ì „ì²˜ë¦¬/ì¸ë±ìŠ¤ ìƒì„± ë‹¨ê³„ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ. (ì˜¤ì§ ë¡œë”©+ê²€ìƒ‰)
#   - ê²€ìƒ‰ ì ìˆ˜ëŠ” ì˜ë¯¸ë¡ (ì„ë² ë”©) + í‚¤ì›Œë“œ(IDF) ê°€ì¤‘ í•©ì˜ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹.
#   - LLMì€ VARCO-VISIONì„ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ í˜¸ì¶œ(ì´ë¯¸ì§€ ì…ë ¥ ì—†ìŒ).
#   - ì½”ë“œ/ì´ë¦„/ë¡œì§ì€ ì›ë³¸ê³¼ ë™ì¼í•˜ë©°, ì£¼ì„ë§Œ ìì„¸íˆ ë³´ê°•.
# ë””ë ‰í„°ë¦¬ ê¸°ëŒ€ êµ¬ì¡°(ì˜ˆ):
#   idx_singlelevel/
#     â”œâ”€ idx_ì „ì²´/
#     â”‚    â”œâ”€ chunks.json      # ì²­í¬ ë³¸ë¬¸/ë©”íƒ€/í‚¤ì›Œë“œ(enriched_text í¬í•¨)
#     â”‚    â”œâ”€ docs.txt         # (ì°¸ê³ ) ì›ë¬¸ ìš”ì•½/ê²½ë¡œ ë“± í…ìŠ¤íŠ¸
#     â”‚    â”œâ”€ vectors.npy      # ë¬¸ì„œ ì„ë² ë”©(ë¬¸ì„œ/ì²­í¬ ê¸°ì¤€)
#     â”‚    â””â”€ index.faiss      # FAISS ì¸ë±ìŠ¤
#     â”œâ”€ idx_ì‹í’ˆìœ„ìƒë²•/
#     â””â”€ ...
# ì‚¬ìš© íë¦„:
#   1) ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ â†’ LLM ë¡œë“œ(ì‹¤íŒ¨ ì‹œ ê²€ìƒ‰/ë¼ë²¨ë§ë§Œ) â†’ ì¸ë±ìŠ¤ ë¡œë“œ
#   2) ì‚¬ìš©ìê°€ ì§ˆì˜ ì…ë ¥ â†’ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ/ì„ íƒ â†’ ê²€ìƒ‰ Top-k ì‚°ì¶œ
#   3) (ì„ íƒ) LLMì— ìƒìœ„ 3ê°œ ì²­í¬ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±í•˜ì—¬ ë‹µë³€ ìƒì„±
#   4) CLIì—ì„œ good/bad ë³µìˆ˜ ì„ íƒ â†’ triplets JSONLë¡œ ë¼ë²¨ ì €ì¥
# =============================================================
import os, re, json, time, sys
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

import numpy as np
import faiss
import torch

from sentence_transformers import SentenceTransformer
from keybert import KeyBERT
from transformers import AutoProcessor

# config.pyì—ì„œ vLLM client ê°€ì ¸ì˜¤ê¸°
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from config import get_vllm_varco_client

SCRIPT_DIR = Path(__file__).resolve().parent
EMBED_MODEL_NAME = "dragonkue/BGE-m3-ko"  # ì„ë² ë”© ëª¨ë¸ ì´ë¦„
LLM_MODEL_ID     = "NCSOFT/VARCO-VISION-2.0-14B"  # LLM ëª¨ë¸ ID

# ë‚ ì§œë³„ ì¸ë±ìŠ¤ ë¡œë“œ (indexes/manual/YYYY-MM-DD êµ¬ì¡°)
# ê³ ì •: ì‹¤ì œ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë¡œ ì„¤ì •
INDEX_DATE = "2025-11-11"  # ì¸ë±ìŠ¤ ìƒì„± ë‚ ì§œ (ê³ ì •)
PARENT_DIR       = str(SCRIPT_DIR.parent / "00_data" / "input" / "indexes" / "manual" / INDEX_DATE)  # ì¸ë±ìŠ¤ ë£¨íŠ¸
TRIPLET_JSONL    = str(SCRIPT_DIR.parent / "00_data" / "output" / "training_data" / "triplets_group_bgem3.jsonl")  # ë¼ë²¨ ëˆ„ì  ì €ì¥ ê²½ë¡œ(JSONL)
SAVE_LOG         = str(SCRIPT_DIR.parent / "00_data" / "output" / "logs" / "result.txt")
# -------------------- ì „ì—­ ëª¨ë¸ --------------------
# GPU ë©”ëª¨ë¦¬ ìºì‹œ ë¹„ìš°ê¸°
torch.cuda.empty_cache()

# KeyBERT, bge-m3: Noneìœ¼ë¡œ ì´ˆê¸°í™” (GlobalBootì—ì„œ ì£¼ì…ë˜ê±°ë‚˜, ë‹¨ë… ì‹¤í–‰ ì‹œ ë¡œë“œ)
# GlobalBoot ì‚¬ìš© ì‹œ ì´ ë³€ìˆ˜ë“¤ì€ GlobalBoot.__init__()ì—ì„œ ë®ì–´ì”Œì›Œì§
kw_model = None
embed_model = None

# ë‹¨ë… ì‹¤í–‰ í™•ì¸ í•¨ìˆ˜ (GlobalBootì—ì„œ í˜¸ì¶œë˜ì§€ ì•Šì„ ë•Œë§Œ ë¡œë“œ)
def _init_models_if_needed():
    """ë‹¨ë… ì‹¤í–‰ ëª¨ë“œì¼ ë•Œë§Œ ëª¨ë¸ ë¡œë“œ"""
    global kw_model, embed_model
    
    if kw_model is None:
        print("[INFO] KeyBERT ë¡œë“œ ì¤‘ (ë‹¨ë… ì‹¤í–‰ ëª¨ë“œ)...")
        kw_model = KeyBERT("paraphrase-multilingual-MiniLM-L12-v2")
    
    if embed_model is None:
        print("[INFO] BGE-m3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ (ë‹¨ë… ì‹¤í–‰ ëª¨ë“œ)...")
        from config import USE_REMOTE_EMBEDDING, EmbeddingModelWrapper
        
        if USE_REMOTE_EMBEDDING:
            print("   ì›ê²© ì„ë² ë”© ì„œë²„ ì‚¬ìš©")
            embed_model = EmbeddingModelWrapper(local_model=None, use_remote=True)
        else:
            print("   ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ")
            local_model = SentenceTransformer(EMBED_MODEL_NAME)
            embed_model = EmbeddingModelWrapper(local_model=local_model, use_remote=False)

# -------------------- ìœ í‹¸  --------------------
# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬/í‚¤ì›Œë“œ ì¶”ì¶œ/ì†ŒìŠ¤ ì •ë¦¬/ê°„ë‹¨ ë¡œê¹… ìœ í‹¸ë¦¬í‹°.

def remove_josa(word: str) -> str:
    # ëª©ì : í•œêµ­ì–´ ì¡°ì‚¬ ì œê±°(ì–´ì ˆ ë§ë‹¨) â†’ IDF ë§¤ì¹­/í‚¤ì›Œë“œ êµì§‘í•© í’ˆì§ˆ í–¥ìƒ
    # ì…ë ¥: word(ë‹¨ì¼ í† í°)
    # ë°˜í™˜: ë§ë‹¨ ì¡°ì‚¬ ì œê±°ëœ ë¬¸ìì—´(ì—†ìœ¼ë©´ ì›ë¬¸)
    for j in ['ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì—','ì˜','ì™€','ê³¼','ë„','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','í•œí…Œ','ë¶€í„°','ê¹Œì§€','ë§Œ','ë³´ë‹¤','ì²˜ëŸ¼','ì¡°ì°¨','ë§ˆì €']:
        if word.endswith(j):
            return word[:-len(j)]
    return word

def get_keywords(text: str, ratio: float = 1, max_k: int = 30, min_k: int = 5) -> List[str]:
    # ëª©ì : ì§ˆì˜ ê¸°ë°˜ í‚¤ì›Œë“œ ì§‘í•© ìƒì„±(top_n ìë™ ì‚°ì •)
    # íŒŒë¼ë¯¸í„°:
    #  - ratio: ë³¸ë¬¸ ê¸¸ì´ì— ë”°ë¥¸ top_k ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„°(ê²½í—˜ì ), max/minìœ¼ë¡œ í´ë¦¬í•‘
    #  - max_k/min_k: ìƒ/í•˜í•œ
    # ë°˜í™˜: ì¡°ì‚¬ ì œê±°ëœ ìœ ë‹ˆí¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸(ìˆœì„œ ë¹„ë³´ì¥)
    # ì˜ˆì™¸: ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
    est_k = int(len(text) * ratio / 10)
    top_k = max(min_k, min(est_k, max_k))
    try:
        kws = kw_model.extract_keywords(text, top_n=top_k)
    except Exception:
        return []
    return list({remove_josa(k[0]) for k in kws if isinstance(k, (list, tuple)) and k and isinstance(k[0], str)})

def preprocess_query(q: str) -> str:
    # ì„¤ê³„: ì¶”ê°€ ì •ê·œí™”/í† í°í™” ë“±ì€ í•˜ì§€ ì•ŠìŒ(ì›ë¬¸ ì§ˆì˜ë¡œ ì„ë² ë”©)
    return q.strip()

# -------------------- ì¹´í…Œê³ ë¦¬/ë§¤í•‘ --------------------
# desired_categories: ì‚¬ìš©ì ì„ íƒ/ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ì— ì“°ì¼ ì¹´í…Œê³ ë¦¬ ì´ë¦„ ëª©ë¡(ë””ë ‰í„°ë¦¬ëª… ê¸°ë°˜)
# mapping           : "ë²ˆí˜¸ ë¬¸ìì—´" â†’ ì¹´í…Œê³ ë¦¬ëª… ë§µí•‘
# category_embeddings: ì¹´í…Œê³ ë¦¬ëª… ìì²´ë¥¼ ë¬¸ì¥ ì„ë² ë”©í•˜ì—¬ ì§ˆì˜ì™€ ìœ ì‚¬ë„ ê³„ì‚°í•  ë•Œ ì‚¬ìš©
# ì´ˆê¸°í™”ëŠ” init_rag_from_savedì—ì„œ ìˆ˜í–‰.

desired_categories: List[str] = []
mapping: Dict[str, str] = {}
category_embeddings: Optional[np.ndarray] = None

# ë§¤ë‰´ì–¼ RAGìš©: parent_dir(=ì•„ì¹´ì´ë¸Œ ë£¨íŠ¸)ì—ì„œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¶”ì¶œ
#  - ë””ë ‰í„°ë¦¬ëª…: idx_<ì¹´í…Œê³ ë¦¬í‘œì‹œì œëª©>
#  - "ì „ì²´"ëŠ” idx_all ì‚¬ìš©
def _scan_categories(parent_dir: str) -> List[str]:
    # ëª©ì : ë£¨íŠ¸ì—ì„œ idx_<name> íŒ¨í„´ ë””ë ‰í„°ë¦¬ë“¤ì„ ì°¾ì•„ ì¹´í…Œê³ ë¦¬ëª… ë¦¬ìŠ¤íŠ¸ ìƒì„±
    cats: List[str] = []
    if not os.path.isdir(parent_dir):
        return cats
    for d in sorted(os.listdir(parent_dir)):
        if d.startswith("idx_") and d != "idx_all":
            cats.append(d[4:])
    return cats

# -------------------- IDF  --------------------
def compute_idf(chunks: List[Dict[str, Any]]) -> Dict[str, float]:
    # ëª©ì : ì²­í¬ì˜ keywords í•„ë“œë¥¼ ì´ìš©í•´ ê°„ë‹¨ IDF ê³„ì‚°
    # ìˆ˜ì‹: idf = log((N+1)/(df+1)) + 1  (ìŠ¤ë¬´ë”©)
    import math
    N = len(chunks)
    df: Dict[str, int] = {}
    for ch in chunks:
        for kw in set(ch.get('keywords', [])):
            df[kw] = df.get(kw, 0) + 1
    return {kw: math.log((N + 1) / (cnt + 1)) + 1 for kw, cnt in df.items()}

# -------------------- ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë”  --------------------
def load_saved_category(cat: str,
                       parent_dir: str = PARENT_DIR) -> Dict[str, Any]:
    # ì…ë ¥: cat("ì „ì²´" ë˜ëŠ” ì¹´í…Œê³ ë¦¬ëª…), parent_dir(ì¸ë±ìŠ¤ ë£¨íŠ¸)
    # ë™ì‘: FAISS ì¸ë±ìŠ¤/ì²­í¬ ë¡œë“œ + embedding_text ë°°ì—´(docs) êµ¬ì„± + IDF ì‚¬ì „ ê³„ì‚°
    # ë°˜í™˜: ê²€ìƒ‰ì— í•„ìš”í•œ êµ¬ì„±ìš”ì†Œ dict(model/index/chunks/docs/IDF/idx_dir)
    if cat == "ì „ì²´":
        save_dir = os.path.join(parent_dir, "idx_all")
    else:
        save_dir = os.path.join(parent_dir, f"idx_{cat}")
    ip = os.path.join(save_dir, "index.faiss")
    jp = os.path.join(save_dir, "chunks.json")
    if not (os.path.isfile(ip) and os.path.isfile(jp)):
        raise FileNotFoundError(f"[{cat}] ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {save_dir}")
    index = faiss.read_index(ip)
    with open(jp, "r", encoding="utf-8") as f:
        chunks = json.load(f)
    docs = [c.get("embedding_text", "") for c in chunks]
    cfg = {
        "model": embed_model,
        "index": index,
        "chunks": chunks,
        "docs": docs,
        "IDF": compute_idf(chunks),
        "idx_dir": save_dir,  # ë³¸ë¬¸ ë¡œë“œë¥¼ ìœ„í•œ ë””ë ‰í„°ë¦¬ ë³´ê´€
    }
    return cfg

# ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™”
def init_rag_from_saved(parent_dir: str) -> Dict[str, Dict[str, Any]]:
    # ëª©ì : ë””ë ‰í„°ë¦¬ ìŠ¤ìº”ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ êµ¬ì„± + ë²ˆí˜¸ ë§¤í•‘ + ì¹´í…Œê³ ë¦¬ ì„ë² ë”© ì‚¬ì „ê³„ì‚° + ì¸ë±ìŠ¤ ë¡œë“œ
    # ë°˜í™˜: ì¹´í…Œê³ ë¦¬ëª… â†’ ì¸ë±ìŠ¤ êµ¬ì„±ìš”ì†Œ dict
    global desired_categories, mapping, category_embeddings
    cat_indices: Dict[str, Dict[str, Any]] = {}

    # ë””ë ‰í„°ë¦¬ ìŠ¤ìº”ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ì±„ìš°ê¸°
    desired_categories = _scan_categories(parent_dir)
    mapping = {str(i): cat for i, cat in enumerate(["ì „ì²´"] + desired_categories)}
    if desired_categories:
        category_embeddings = embed_model.encode(desired_categories, normalize_embeddings=True, convert_to_tensor=False)
    else:
        category_embeddings = np.zeros((0, 1), dtype=np.float32)

    # ê°œë³„ ì¹´í…Œê³ ë¦¬ ë¡œë“œ(ìˆì„ ë•Œë§Œ)
    for cat in desired_categories:
        try:
            cat_indices[cat] = load_saved_category(cat, parent_dir)
        except FileNotFoundError:
            pass
    # ì „ì²´(idx_all) ë¡œë“œ ì‹œë„
    try:
        cat_indices["ì „ì²´"] = load_saved_category("ì „ì²´", parent_dir)
    except FileNotFoundError:
        if desired_categories:
            # ì „ì²´ê°€ ì—†ìœ¼ë©´ ì²« ì¹´í…Œê³ ë¦¬ë¥¼ ëŒ€ì²´ë¡œ ì‚¬ìš©
            cat_indices["ì „ì²´"] = cat_indices.get(desired_categories[0])
    return cat_indices

# -------------------- ì¹´í…Œê³ ë¦¬ ì¶”ì²œ  --------------------
def parse_category_input(inp: str) -> Optional[str]:
    # ëª©ì : ì‚¬ìš©ìê°€ ë²ˆí˜¸/ì´ë¦„ í˜¼í•© ì…ë ¥ ì‹œ ìœ ì—° ë§¤í•‘
    # ì…ë ¥ ì˜ˆ: "3", "3ì‹í’ˆ", "HACCP", "ì „ì²´"
    inp = inp.strip()
    if inp in mapping:
        return mapping[inp]
    m = re.match(r"(\d+)(.+)", inp)
    if m and m.group(1) in mapping:
        return mapping[m.group(1)]
    if inp in mapping.values():
        return inp
    return None
#ìˆ˜ì •
def parse_multi_category_input(inp: str) -> List[str]:
    """
    ì‰¼í‘œ/ê³µë°±ìœ¼ë¡œ ì—¬ëŸ¬ ê°œ ì…ë ¥ ì§€ì›.
    - '0' ë˜ëŠ” 'ì „ì²´'ê°€ í¬í•¨ë˜ë©´ ['ì „ì²´']ë§Œ ë°˜í™˜.
    - ì¤‘ë³µ ì œê±°, ì…ë ¥ ìˆœì„œ ìœ ì§€.
    """
    if not inp:
        return []
    tokens = [t for t in re.split(r'[,\s]+', inp.strip()) if t]
    if any(t == '0' or t == 'ì „ì²´' for t in tokens):
        return ['ì „ì²´']

    seen, out = set(), []
    for t in tokens:
        cat = parse_category_input(t)  # ê¸°ì¡´ ë‹¨ì¼ íŒŒì„œ í™œìš©
        if cat and cat not in seen:
            seen.add(cat)
            out.append(cat)
    return out

# def classify_category(query: str, sem_threshold: float = 0.4) -> List[str]:
#     # ëª©ì : ì§ˆì˜ì™€ ì¹´í…Œê³ ë¦¬ëª… ì„ë² ë”© ê°„ ë‚´ì  ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ í›„ë³´ ì¶”ì²œ
#     # íŒŒë¼ë¯¸í„°: sem_threshold â€” ìœ ì‚¬ë„ ì„ê³„(0~1). ë‚®ì¶œìˆ˜ë¡ í›„ë³´ ë‹¤ìˆ˜ ì¶”ì²œ.
#     # ë°˜í™˜: ì¶”ì²œ ì¹´í…Œê³ ë¦¬ëª… ë¦¬ìŠ¤íŠ¸(ìµœëŒ€ 2ê°œ)
#     q_vec = embed_model.encode([query], normalize_embeddings=True)[0]
#     sims = np.dot(category_embeddings, q_vec)
#     idx = np.where(sims >= sem_threshold)[0]
#     if idx.size == 0:
#         return []
#     idx = idx[np.argsort(sims[idx])[::-1]][:2]
#     return [desired_categories[i] for i in idx]

def classify_category(query: str, sem_threshold: float = 0.4) -> List[str]:
    """
    ëª©ì : ì„ë² ë”© ìœ ì‚¬ë„ ëŒ€ì‹  'RAG ê²€ìƒ‰ ì ìˆ˜'ë¡œ ìƒìœ„ í›„ë³´ ì¹´í…Œê³ ë¦¬(ìµœëŒ€ 2ê°œ) ì¶”ì²œ
    - main()ì—ì„œ init_rag_from_saved(...)ê°€ ë§Œë“  cat_indicesë¥¼ globals()ë¡œ ì°¸ì¡°
    - 'ì „ì²´' ì¸ë±ìŠ¤ëŠ” ë¼ìš°íŒ… ëŒ€ìƒì—ì„œ ì œì™¸
    - ê° ì¹´í…Œê³ ë¦¬ì—ì„œ retrieve_docs(..., top_k=3)ì˜ ìµœê³  scoreë¥¼ ëŒ€í‘œ ì ìˆ˜ë¡œ ì‚¬ìš©
    - score>0ë§Œ í›„ë³´ë¡œ ì±„íƒ, ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ(ë™ì  ì‹œ ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœ) ì •ë ¬
    """
    indices = globals().get("cat_indices")
    if not isinstance(indices, dict):
        return []

    uq = preprocess_query(query)
    scored: list[tuple[str, float]] = []

    for cat, cfg in indices.items():
        if cat == "ì „ì²´":
            continue
        try:
            results = retrieve_docs(
                uq,
                cfg["model"], cfg["index"], cfg["docs"], cfg["chunks"], cfg["IDF"],
                top_k=3
            )
            best_score = max((r.get("score", 0.0) for r in results), default=0.0)
        except Exception:
            best_score = 0.0

        if best_score > 0.0:
            scored.append((cat, best_score))

    scored.sort(key=lambda x: (-x[1], x[0]))
    return [c for c, _ in scored[:2]]
#
# -------------------- ë³¸ë¬¸ ë¡œë” --------------------
# idx_dir ê°€ë¦¬í‚¤ëŠ” í´ë”ì˜ ìƒìœ„ í´ë”ì— <ì œëª©>_chunks.json(í’€ í…ìŠ¤íŠ¸)ê°€ ìˆë‹¤ê³  ê°€ì •.
# í˜„ì¬ êµ¬í˜„ì—ì„œëŠ” idx/chunks.json ì•ˆì— textê°€ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆì–´ ì§ì ‘ ì‚¬ìš©.

def _title_from_idx_dir(idx_dir: str) -> str:
    # ëª©ì : idx_<ì œëª©> ë””ë ‰í„°ë¦¬ëª…ì—ì„œ ì œëª©ë§Œ ì¶”ì¶œ
    return os.path.basename(idx_dir)[4:]

def resolve_text_for_chunk(candidate: Dict[str, Any], idx_dir: str) -> Optional[str]:
    # ëª©ì : í”„ë¡¬í”„íŠ¸ìš© ë³¸ë¬¸ í…ìŠ¤íŠ¸ í™•ë³´ ìš°ì„ ìˆœìœ„
    # 1) candidate['text']ê°€ list/dictë©´ JSON ì§ë ¬í™”í•˜ì—¬ ì‚¬ìš©(ì›ë³¸ ë³´ì¡´)
    # 2) ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    # 3) ëª¨ë‘ ì—†ìœ¼ë©´ embedding_textë¡œ ëŒ€ì²´(ìµœí›„ ìˆ˜ë‹¨)
    tx = candidate.get("text")
    if isinstance(tx, (list, dict)):
        return json.dumps(tx, ensure_ascii=False)
    if isinstance(tx, str) and tx.strip():
        return tx
    # ì•ˆì „ì¥ì¹˜: ë¹„ì–´ìˆìœ¼ë©´ embedding_textë¡œ ëŒ€ì²´
    return candidate.get("embedding_text", "")

# -------------------- ê²€ìƒ‰: í•˜ì´ë¸Œë¦¬ë“œ(semantic + IDF) --------------------
def retrieve_docs(query: str, model: SentenceTransformer, index, docs, chunks, IDF,
                  alpha: float = 0.9, top_k: int = 5, idx_dir: Optional[str] = None) -> List[Dict[str, Any]]:
    # ëª©ì : ì„ë² ë”© ìœ ì‚¬ë„(sem)ì™€ í‚¤ì›Œë“œ IDF ì ìˆ˜(ks)ë¥¼ ê²°í•©í•´ ìµœì¢… ìƒìœ„ top_k ì²­í¬ ì„ íƒ
    # ìŠ¤ì½”ì–´: score = alpha * sem + (1 - alpha) * ks, ë‘ ì ìˆ˜ëŠ” [0,1]ë¡œ ì •ê·œí™” ê°€ì •
    # íŒŒë¼ë¯¸í„°:
    #   - docs : embedding_text ë¦¬ìŠ¤íŠ¸(ì¸ë±ìŠ¤ ìˆœì„œì™€ 1:1)
    #   - chunks: ì²­í¬ ì›ë³¸ ë¦¬ìŠ¤íŠ¸(ê²€ìƒ‰ í›„ ê²°ê³¼ ë§¤í•‘/ë³¸ë¬¸ ì£¼ì…ì— ì‚¬ìš©)
    #   - IDF : í‚¤ì›Œë“œë³„ idf ê°€ì¤‘ì¹˜ ì‚¬ì „(ì²­í¬ keywordsë¡œ ê³„ì‚°)
    #   - idx_dir: ì„ íƒ ì‹œ ë³¸ë¬¸(text) ì£¼ì…ì„ ìœ„í•´ ë””ë ‰í„°ë¦¬ íŒíŠ¸ ì „ë‹¬
    # ë°˜í™˜: ìƒìœ„ ê²°ê³¼ ì²­í¬ ë¦¬ìŠ¤íŠ¸(ë³¸ë¬¸ text ì£¼ì… ì™„ë£Œ)
    qv = model.encode([query], normalize_embeddings=True)[0]
    dists, I = index.search(np.array([qv]), len(docs))
    dists, I = dists[0], I[0]
    sem = np.clip(dists, 0, 1)
    qk = set(get_keywords(query))
    ks = np.array([
        sum(IDF.get(kw, 1.0) for kw in (qk & set(chunks[i].get('keywords', [])))) for i in I
    ], dtype=np.float32)
    if ks.max() > 0:
        ks /= (ks.max() + 1e-6)
    scores = alpha * sem + (1 - alpha) * ks
    top_indices = I[np.argsort(scores)[::-1][:top_k]]
    
    # ì ìˆ˜ë¥¼ ì²­í¬ì— ì¶”ê°€
    selected = []
    for idx in top_indices:
        chunk = chunks[idx].copy()
        chunk['score'] = round(float(scores[np.where(I == idx)[0][0]]), 2)  # í•´ë‹¹ ì²­í¬ì˜ ì ìˆ˜ ì¶”ê°€
        selected.append(chunk)
   
    # ë§¤ë‰´ì–¼ì€ ëª¨ë“  ê²½ìš°ì— embedding_text ì‚¬ìš© (í‰íƒ„í™”ëœ ë§ˆí¬ë‹¤ìš´)
    for ch in selected:
        if 'embedding_text' in ch:
            ch['text'] = ch['embedding_text']
    
    return selected

# -------------------- í”„ë¡¬í”„íŠ¸ (ë§¤ë‰´ì–¼ ì§€ì¹¨) --------------------
def build_chatml_prompt(question: str, results: List[Dict[str, Any]], max_blocks: int = 2, wrap_width: int = 80) -> str:
    # ëª©ì :  ë§¤ë‰´ì–¼ ë¬¸ë§¥/ì§€ì‹œì‚¬í•­ì— ë§ì¶˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
    # íŒŒë¼ë¯¸í„°:
    #   - max_blocks: í”„ë¡¬í”„íŠ¸ì— ì‹¤ì„ ìƒìœ„ ë¸”ë¡ ê°œìˆ˜(ê¸°ë³¸ 2)
    #   - wrap_width: (ë¯¸ì‚¬ìš©) ì¤„ë°”ê¿ˆ í­ ìë¦¬
    # êµ¬ì„±:
    #   - system: ë‹µë³€ ê·œì¹™(í•µì‹¬ ìš”ì•½, ì œê³µ ë¸”ë¡ ê·¼ê±° ëª…ì‹œ)
    #   - user  : ì§ˆë¬¸ + ì„ íƒ ë¸”ë¡(context)
    blocks = []
    for i, ch in enumerate(results[:max_blocks], start=1):
        title = ch.get("source", "")
        body = ch.get("text", "")
        blocks.append(f"{i}. {title}\n{body}")
    context = "\n\n".join(blocks)
    prompt = f"""<|im_start|>system
ë‹¹ì‹ ì€ HACCP/ì‹í’ˆì•ˆì „ ë§¤ë‰´ì–¼ì— ì •í†µí•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ì—ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë§¤ë‰´ì–¼ ë°œì·Œ ë¸”ë¡ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ê·œì¹™:
1) ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ í•µì‹¬ë§Œ ìš”ì•½í•˜ì„¸ìš”.
2) ì˜¤ì§ ì œê³µëœ ë¸”ë¡ì— ê·¼ê±°í•´ ëª…í™•íˆ ë‹µë³€í•˜ì„¸ìš”. 

[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
1. ìš”ì•½:
(ì—¬ê¸°ì— í•µì‹¬ ìš”ì•½)
2. ë‹µë³€:
(ì—¬ê¸°ì— ë‹µë³€ ë‚´ìš©)
<|im_end|>
<|im_start|>user
[ì§ˆë¬¸]
{question}

[ë¸”ë¡]
{context}
<|im_end|>
<|im_start|>assistant
""".strip()
    return prompt

# -------------------- LLM (vLLM ì„œë²„ ì‚¬ìš©) --------------------
def load_llm(model_id: str = LLM_MODEL_ID):
    """
    vLLM serving framework í™œìš©ì„ ìœ„í•´ ìˆ˜ì •ë¨
    
    ê¸°ì¡´ê³¼ ë‹¬ë¦¬ ëª¨ë¸ì„ ì§ì ‘ ë¡œë“œí•˜ì§€ ì•Šê³  í”„ë¡œì„¸ì„œë§Œ ë¡œë“œ
    ì‹¤ì œ ëª¨ë¸ì€ vLLM ì„œë²„ì—ì„œ ì„œë¹™ë¨
    """
    processor = AutoProcessor.from_pretrained(model_id)
    return None, processor

def generate_llm_response(model, processor, conversation, max_new_tokens: int = 1024):
    """
    vLLM serving framework í™œìš©ì„ ìœ„í•´ ìˆ˜ì •ë¨
    
    ëª¨ë¸ì— ì§ì ‘ ì…ë ¥í•˜ì§€ ì•Šê³  vLLM clientë¡œ localhost:8400/v1ì— ìš”ì²­
    """
    # ChatML í…œí”Œë¦¿ ì ìš© (í† í¬ë‚˜ì´ì¦ˆëŠ” ì„œë²„ì—ì„œ ì²˜ë¦¬)
    rendered_prompt = processor.apply_chat_template(
        conversation, add_generation_prompt=True, tokenize=False
    )

    # í† í° ê¸¸ì´ ê³„ì‚° (max_tokens ì„¤ì •ìš©)
    input_len = len(processor.tokenizer(rendered_prompt)["input_ids"])
    
    # max_tokens ì•ˆì „í•˜ê²Œ ê³„ì‚°
    MAX_CONTEXT_LENGTH = 4096
    RESERVED_OUTPUT_TOKENS = 1024
    MIN_OUTPUT_TOKENS = 100
    
    available_tokens = MAX_CONTEXT_LENGTH - input_len
    max_tokens = min(available_tokens, RESERVED_OUTPUT_TOKENS)
    
    if max_tokens < MIN_OUTPUT_TOKENS:
        # ì…ë ¥ì´ ë„ˆë¬´ ê¸¸ì–´ì„œ ì¶œë ¥ ê³µê°„ì´ ë¶€ì¡±í•œ ê²½ìš°
        print(f"âš ï¸ ê²½ê³ : ì…ë ¥ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({input_len} tokens). ìµœì†Œ ì¶œë ¥ í† í° ë³´ì¥ ë¶ˆê°€.")
        max_tokens = max(1, available_tokens)  # ìµœì†Œí•œ 1 í† í°ì€ ë³´ì¥

    # vLLM ì„œë²„ì— ìš”ì²­
    client = get_vllm_varco_client()
    start = time.time()
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL_ID,
            messages=conversation,
            max_tokens=max_tokens
        )
        elapsed = time.time() - start
        output_text = response.choices[0].message.content
        return {"rendered_prompt": rendered_prompt, "output": output_text.strip(), "elapsed": elapsed}
    except Exception as e:
        elapsed = time.time() - start
        print(f"âŒ vLLM ì„œë²„ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return {"rendered_prompt": rendered_prompt, "output": f"[ì˜¤ë¥˜] vLLM ì„œë²„ ì‘ë‹µ ì‹¤íŒ¨: {str(e)}", "elapsed": elapsed}

# -------------------- Triplet ë¼ë²¨ë§  --------------------
def save_group_jsonl(query: str,
                     positives: List[str],
                     negatives: List[str],
                     pos_sources: Optional[List[str]] = None,
                     neg_sources: Optional[List[str]] = None,
                     extra_meta: Optional[dict] = None,
                     out_path: str = TRIPLET_JSONL):
    # ëª©ì : í•œ ë²ˆì˜ ì¸í„°ë™ì…˜ì—ì„œ ì„ íƒëœ good/bad ë³¸ë¬¸ë“¤ì„ í•˜ë‚˜ì˜ JSONL ë ˆì½”ë“œë¡œ ì €ì¥
    # í•„ë“œ:
    #  - query/positives/negatives
    #  - meta.timestamp / pos_sources / neg_sources / extra_meta(ì„ íƒ)
    def _clean(s: str) -> str:
        s = s.replace("\t", " ").replace("\r", " ").replace("\n", " ")
        return re.sub(r"\s+", " ", s).strip()
    rec = {
        "query": _clean(query),
        "positives": [_clean(p) for p in positives if p and p.strip()],
        "negatives": [_clean(n) for n in negatives if n and n.strip()],
        "meta": {"timestamp": datetime.now().isoformat(timespec="seconds")}
    }
    if pos_sources: rec["meta"]["pos_sources"] = [_clean(x) for x in pos_sources]
    if neg_sources: rec["meta"]["neg_sources"] = [_clean(x) for x in neg_sources]
    if extra_meta:  rec["meta"].update(extra_meta)
    # í´ë” ìë™ ìƒì„±
    Path(out_path).parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")

# ì‚¬ìš©ì ì„ íƒí˜• ë¼ë²¨ ì¸í„°ë™ì…˜(ë§¤ë‰´ì–¼ê³¼ ë™ì¼ UX ìœ ì§€)
def interactive_label_group(question: str,
                            candidates: List[Dict[str, Any]],
                            llm_used_n: int = 2) -> bool:
    # ëª©ì : CLIì—ì„œ ë‹¤ì¤‘ good/bad í›„ë³´ ë²ˆí˜¸ë¥¼ ë°›ì•„ save_group_jsonlì— ê¸°ë¡
    # ë™ì‘:
    #  - í›„ë³´ ëª©ë¡ sourceë¥¼ ì¶œë ¥í•˜ì—¬ ì‚¬ëŒì´ ë¹ ë¥´ê²Œ ì‹ë³„
    #  - "good ë²ˆí˜¸", "bad ë²ˆí˜¸"ë¥¼ ì½¤ë§ˆë¡œ ì…ë ¥ë°›ì•„ ì¤‘ë³µ/ë²”ìœ„ ì²´í¬
    #  - ì„ íƒì´ ìœ íš¨í•˜ë©´ positives/negativesë¥¼ ì €ì¥
    if not candidates:
        print("âš ï¸ ë¼ë²¨ë§í•  í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    print("\n===== [ë¼ë²¨ë§] ë‹¤ì¤‘ good/bad ì„ íƒ =====")
    print(f"[ì§ˆë¬¸]\n{question}\n")
    print("[í›„ë³´ ëª©ë¡]")
    for i, ch in enumerate(candidates, start=1):
        title = ch.get("source", "")
        print(f"{i}) {title}\n")
    def parse_indices(s: str, N: int) -> List[int]:
        if not s: return []
        vals = []
        for tok in s.split(","):
            tok = tok.strip()
            if tok.isdigit():
                v = int(tok)
                if 1 <= v <= N: vals.append(v-1)
        return sorted(set(vals))
    print("ì˜ˆ) good: 1,3   bad: 2,5   (ë¹„ìš°ë©´ ê±´ë„ˆëœ€)")
    pos_idx = parse_indices(input("good ë²ˆí˜¸: ").strip(), len(candidates))
    neg_idx = parse_indices(input("bad  ë²ˆí˜¸: ").strip(), len(candidates))
    if not pos_idx and not neg_idx:
        print("â¡ï¸ ì…ë ¥ì´ ì—†ì–´ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False
    overlap = set(pos_idx) & set(neg_idx)
    if overlap:
        print(f"âš ï¸ ê²¹ì¹˜ëŠ” ë²ˆí˜¸ ì œì™¸: {[i+1 for i in overlap]}")
        pos_idx = [i for i in pos_idx if i not in overlap]
        neg_idx = [i for i in neg_idx if i not in overlap]
    positives = [candidates[i].get("text", candidates[i].get("embedding_text", "")) for i in pos_idx]
    negatives = [candidates[i].get("text", candidates[i].get("embedding_text", "")) for i in neg_idx]
    pos_srcs  = [candidates[i].get("source", "") for i in pos_idx]
    neg_srcs  = [candidates[i].get("source", "") for i in neg_idx]
    if not positives and not negatives:
        print("âš ï¸ ìœ íš¨í•œ ì„ íƒì´ ì—†ì–´ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False
    save_group_jsonl(question, positives, negatives, pos_srcs, neg_srcs,
                     extra_meta={"retrieved_topk": len(candidates), "llm_used_topn": llm_used_n})
    print(f"âœ… ê·¸ë£¹ ì €ì¥ ì™„ë£Œ â†’ {TRIPLET_JSONL}")
    return True

# -------------------- ë©”ì¸  --------------------
def main():
    # 0) ë‹¨ë… ì‹¤í–‰ ëª¨ë“œì¼ ë•Œ ëª¨ë¸ ë¡œë“œ
    _init_models_if_needed()
    
    # 1) LLM ë¡œë“œ 
    # - ì‹¤íŒ¨ ì‹œ ê²€ìƒ‰/ë¼ë²¨ë§ë§Œ ìˆ˜í–‰(USE_LLM=False)
    try:
        model_llm, processor = load_llm(LLM_MODEL_ID)
        USE_LLM = True
        print("[INFO] LLM ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"[ê²½ê³ ] LLM ë¡œë“œ ì‹¤íŒ¨ â†’ ê²€ìƒ‰/ë¼ë²¨ë§ë§Œ ì‚¬ìš©: {e}")
        model_llm = processor = None
        USE_LLM = False

    # 2) ì¸ë±ìŠ¤ ë£¨íŠ¸ ì§€ì •
    # - ë””ë ‰í„°ë¦¬ ìŠ¤ìº” â†’ mapping/desired_categories/category_embeddings êµ¬ì„±
    # - ê° ì¹´í…Œê³ ë¦¬/ì „ì²´ ì¸ë±ìŠ¤ ë¡œë“œ
    parent_dir = PARENT_DIR
    cat_indices = init_rag_from_saved(parent_dir)
#ìˆ˜ì •
    globals()['cat_indices'] = cat_indices
#
    # 3) ê°„ë‹¨ ì„¸ì…˜ ë¡œê·¸
    # - ì´í›„ ì§ˆì˜/ì¹´í…Œê³ ë¦¬/í”„ë¡¬í”„íŠ¸/ì‘ë‹µ/ì†Œìš”ì‹œê°„ ë“±ì„ appendí•˜ì—¬ íšŒê³  ê°€ëŠ¥
    log_path = SAVE_LOG
    with open(log_path, "a", encoding="utf-8") as log:
        log.write(f"\n\n===== VARCO-VISION + RAG ì„¸ì…˜ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} =====\n")

    # 4) ëŒ€í™”í˜• ë£¨í”„
    # - exit ì…ë ¥ ì‹œ ì¢…ë£Œ
    # - ì¶”ì²œ ì¹´í…Œê³ ë¦¬ í‘œì¶œ â†’ ë²ˆí˜¸/ì´ë¦„ ì…ë ¥ íŒŒì‹± â†’ ì—†ìœ¼ë©´ ì¶”ì²œ/ì „ì²´ë¡œ í´ë°±
    # - retrieve_docsë¡œ top-5 ê²€ìƒ‰ â†’ ìƒìœ„ 2ê°œë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± â†’ LLM ìƒì„±(ê°€ëŠ¥ ì‹œ)
    # - ë¼ë²¨ë§ ë‹¨ê³„ë¡œ ì´ì–´ì§
    while True:
        q = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit) >> ").strip()
        if not q:
            continue
        if q.lower() == "exit":
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        print("\n=== ì „ì²´ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ===")
        for i in range(0, len(desired_categories)+1):
            print(f"{i}) {mapping.get(str(i), '')}", end="    ")
            if (i % 6) == 5:
                print()
        print("\n")

        # ì¶”ì²œ í›„ë³´
        candidates_cat = classify_category(q)
        print("ì¶”ì²œ ì¹´í…Œê³ ë¦¬:")
        for i, cat in enumerate(desired_categories, start=1):
            if cat in candidates_cat:
                print(f"{i}) {cat}")
        print("0) ì „ì²´")

        # choice = input("ë²ˆí˜¸/ì´ë¦„ ì…ë ¥(ë¯¸ì…ë ¥=ì¶”ì²œâ†’ì „ì²´/ì²«ë²ˆì§¸) >> ").strip()
        # sel = parse_category_input(choice) if choice else None
        # if not sel:
        #     sel = candidates_cat[0] if candidates_cat else "ì „ì²´"
        # if sel not in cat_indices or not cat_indices.get(sel):
        #     print(f"[ê²½ê³ ] '{sel}' ì¸ë±ìŠ¤ê°€ ì—†ì–´ 'ì „ì²´'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        #     sel = "ì „ì²´"
#ìˆ˜ì •
        choice = input("ë²ˆí˜¸/ì´ë¦„ (ì‰¼í‘œë¡œ ì—¬ëŸ¬ ê°œ, ë¯¸ì…ë ¥=ì¶”ì²œ ìƒìœ„/ì „ì²´) >> ").strip()

        if choice:
            sel_list = parse_multi_category_input(choice)
        else:
            # ë¯¸ì…ë ¥: ì¶”ì²œì´ ìˆìœ¼ë©´ ìµœëŒ€ 2ê°œ, ì—†ìœ¼ë©´ 'ì „ì²´'
            sel_list = candidates_cat[:2] if candidates_cat else ['ì „ì²´']

        # ë¡œë“œëœ ì¸ë±ìŠ¤ë§Œ ìœ ì§€
        sel_list = [c for c in sel_list if c in cat_indices and cat_indices.get(c)]
        if not sel_list:
            sel_list = ['ì „ì²´']
#

        # cfg = cat_indices[sel]
        # uq = preprocess_query(q)

        # # ê²€ìƒ‰(ë§¤ë‰´ì–¼ í•˜ì´ë¸Œë¦¬ë“œ + ë³¸ë¬¸ ì£¼ì…)
        # results_top5 = retrieve_docs(uq, cfg["model"], cfg["index"], cfg["docs"], cfg["chunks"], cfg["IDF"],
        #                              alpha=0.9, top_k=5, idx_dir=cfg.get("idx_dir"))
        # results_for_prompt = results_top5[:2]  # ë§¤ë‰´ì–¼ í”„ë¡¬í”„íŠ¸ ê¸°ì¤€
#ìˆ˜ì •

        uq = preprocess_query(q)

        def _dedup_by_source(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
            """source ê¸°ì¤€ ì¤‘ë³µ ì œê±°(ì²« ë“±ì¥ ìœ ì§€). í•„ìš” ì—†ìœ¼ë©´ ì œê±° ê°€ëŠ¥."""
            seen, out = set(), []
            for r in rows:
                key = r.get("source")
                if key in seen:
                    continue
                seen.add(key)
                out.append(r)
            return out

        if sel_list == ['ì „ì²´']:
            cfg = cat_indices['ì „ì²´']
            results_top5 = retrieve_docs(
                uq, cfg["model"], cfg["index"], cfg["docs"], cfg["chunks"], cfg["IDF"],
                alpha=0.9, top_k=5, idx_dir=cfg.get("idx_dir")
            )
        else:
            aggregated: List[Dict[str, Any]] = []
            per_cat_k = 3  # ì¹´í…Œê³ ë¦¬ë³„ ëª‡ ê°œì”© ë½‘ì„ì§€ (ì›í•˜ë©´ ì¡°ì ˆ)
            for sel in sel_list:
                cfg = cat_indices[sel]
                part = retrieve_docs(
                    uq, cfg["model"], cfg["index"], cfg["docs"], cfg["chunks"], cfg["IDF"],
                    alpha=0.9, top_k=per_cat_k, idx_dir=cfg.get("idx_dir")
                )
                for r in part:
                    r["category"] = sel  # ë””ë²„ê¹…/ë¡œê·¸ìš© íƒœê·¸
                aggregated.extend(part)

            aggregated = _dedup_by_source(aggregated)
            aggregated.sort(key=lambda x: x.get("score", 0.0), reverse=True)
            results_top5 = aggregated[:5]

        results_for_prompt = results_top5[:2]

        # print(f"\n== ê²€ìƒ‰ ê²°ê³¼ (ì„ íƒ: {', '.join(sel_list)}) ==")
        # for r in results_for_prompt:
        #     cat_tag = f"[{r.get('category')}]" if r.get('category') else ""
        #     print(f"{cat_tag} {r.get('source','')}")


        # # ê°„ë‹¨ ê²°ê³¼ í‘œì‹œ/ë¡œê·¸
        # print(f"\n== {sel} ê²€ìƒ‰ ê²°ê³¼ ==")
        # for r in results_for_prompt:
        #     print(r.get("source", ""))
#ìˆ˜ì •
        print(f"\n== ê²€ìƒ‰ ê²°ê³¼ (ì„ íƒ: {', '.join(sel_list)}) ==")
        for r in results_for_prompt:
            cat_tag = f"[{r.get('category')}]" if r.get('category') else ""
            print(f"{cat_tag} {r.get('source','')}")

        # ë¡œê·¸ë„ sel â†’ sel_listë¡œ êµì²´
        log.write(f"\nğŸ‘¤ ì§ˆë¬¸: {q}\n")
        log.write(f"ğŸ“‚ ì„ íƒ ì¹´í…Œê³ ë¦¬: {', '.join(sel_list)}\n")
#
        # LLM ì‘ë‹µ(í…ìŠ¤íŠ¸ë§Œ)
        if USE_LLM:
            chatml_prompt = build_chatml_prompt(q, results_for_prompt, max_blocks=2, wrap_width=80)
            conversation = [{"role": "user", "content": [{"type": "text", "text": chatml_prompt}]}]
            gen = generate_llm_response(model_llm, processor, conversation, max_new_tokens=1024)
            print(f"\nâœ… LLM ì‘ë‹µ (â± {gen['elapsed']:.2f}s)\n")
            print(gen["output"])
            with open(log_path, "a", encoding="utf-8") as log:
                log.write(f"\nğŸ‘¤ ì§ˆë¬¸: {q}\n")
                log.write(f"ğŸ“‚ ì„ íƒ ì¹´í…Œê³ ë¦¬: {sel}\n")
                for r in results_for_prompt:
                    log.write(f" - {r.get('source','')}\n")
                log.write("\n--- Rendered Prompt ---\n")
                log.write(gen["rendered_prompt"] + "\n")
                log.write(f"\nğŸ¤– VARCO ì‘ë‹µ:\n{gen['output']}\n")
                log.write(f"â± ì†Œìš”ì‹œê°„: {gen['elapsed']:.2f}ì´ˆ\n")
        else:
            print("\n[ì•ˆë‚´] LLM ë¹„í™œì„±/ë¡œë”© ì‹¤íŒ¨ë¡œ ë‹µë³€ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        input("\n(ì—”í„°ë¥¼ ëˆ„ë¥´ë©´ ë¼ë²¨ë§ ë‹¨ê³„ë¡œ ì´ë™í•©ë‹ˆë‹¤) ")
        _ = interactive_label_group(q, results_top5, llm_used_n=min(2, len(results_top5)))

if __name__ == "__main__":
    main()
